[
  {
    "section_title": "Introduction",
    "timestamp_range": "00:00:00 \u2013 00:00:15",
    "level": 2,
    "order": 1,
    "content": "### \ud83c\udfa4 [00:00:00 \u2013 00:00:15] Introduction  \n**Timestamp**: 00:00:00 \u2013 00:00:15\n\n**Key Concepts**  \n- Training data corpus as the foundational knowledge base for the model  \n- The training data is sourced from books, websites, and transcripts  \n- The model is generative and predicts the next most probable token based on training  \n\n**Definitions**  \n- **Corpus of training data**: A finite collection of text data used to train the model, consisting of publicly available sources up to a certain cutoff date  \n- **Generative model**: A model that generates responses by predicting the next most probable token in a sequence  \n\n**Key Facts**  \n- Training data is finite and has a cutoff date  \n- Non-public information is not included in the training data  \n- Model training involves adjusting weights and biases of neurons based on connection strengths  \n\n**Examples**  \n- None mentioned explicitly in this segment  \n\n**Key Takeaways \ud83c\udfaf**  \n- The model\u2019s knowledge is limited to its training corpus, which is finite and time-bound  \n- The generative model functions by predicting tokens sequentially to produce responses  \n- Understanding the source and limitations of training data is crucial to grasping model capabilities and constraints  \n\n---"
  },
  {
    "section_title": "AI models and their knowledge",
    "timestamp_range": "00:00:15 \u2013 00:01:31",
    "level": 2,
    "order": 2,
    "content": "### \ud83c\udfa4 [00:00:15 \u2013 00:01:31] AI models and their knowledge  \n**Timestamp**: 00:00:15 \u2013 00:01:31\n\n**Key Concepts**  \n- AI models are trained on a fixed corpus of data, which forms their body of knowledge.  \n- The training data includes books, websites, and transcripts.  \n- The training dataset is finite and has a cutoff date; it excludes non-public information.  \n- During training, the model\u2019s neurons adjust weights and biases based on the data.  \n- After training, the model generates responses by predicting the most probable next tokens.  \n- To use information outside the pre-trained knowledge, it must be provided as part of the input prompt.\n\n**Definitions**  \n- **Corpus of training data**: The complete set of text data (books, websites, transcripts) used to train the AI model.  \n- **Generative model**: An AI model that produces text by predicting the next most probable token based on learned patterns.  \n- **Weights and biases**: Parameters within the neural network adjusted during training to capture relationships in the data.\n\n**Key Facts**  \n- The training data is finite and has a cutoff date, meaning it does not include any information published after that date.  \n- Non-public information is not included in the training data.  \n- The model generates responses token-by-token based on probability predictions.\n\n**Examples**  \n- None explicitly mentioned, but the process of providing additional data as part of a prompt is implied as a practical approach.\n\n**Key Takeaways \ud83c\udfaf**  \n- AI models rely solely on their pre-trained knowledge and cannot access new or private information unless explicitly provided during a request.  \n- Understanding the limitations of the training data (finite size, cutoff date, public-only) is crucial when using AI models.  \n- To extend the model\u2019s capabilities beyond its training, external information must be included in the prompt or request."
  },
  {
    "section_title": "RAG to the rescue",
    "timestamp_range": "00:01:31 \u2013 00:03:12",
    "level": 2,
    "order": 3,
    "content": "### \ud83c\udfa4 [00:01:31 \u2013 00:03:12] RAG to the rescue  \n**Timestamp**: 00:01:31 \u2013 00:03:12\n\n**Key Concepts**  \n- Pre-trained AI models have limitations because they only know data they were trained on.  \n- To utilize information outside the model\u2019s training data, additional relevant data must be provided at request time.  \n- Retrieval-Augmented Generation (RAG) is the process of retrieving external information to augment the model\u2019s response generation.  \n- The quality and relevance of the retrieved data directly impact the quality of the AI model\u2019s output.  \n- Azure AI Search can be used to retrieve relevant data from various company data sources to feed into the model.\n\n**Definitions**  \n- **Retrieval-Augmented Generation (RAG)**: A method where additional information is retrieved from external sources and added to the prompt to enhance the AI model\u2019s response capabilities.\n\n**Key Facts**  \n- AI models have cutoff dates and do not inherently know company-specific or updated data.  \n- The application first queries the external data source, receives relevant information, then appends it to the prompt sent to the AI model.  \n- Garbage in, garbage out principle applies: poor quality or irrelevant data leads to poor AI responses.  \n- Azure AI Search exposes an API endpoint and creates indexes to facilitate retrieval of relevant data.\n\n**Examples**  \n- Using an internal company database or other knowledge bases as the external data source to retrieve relevant information before querying the AI model.  \n- Azure AI Search acting as the retrieval mechanism to provide relevant data to augment the prompt.\n\n**Key Takeaways \ud83c\udfaf**  \n- AI models alone cannot answer questions about data they were not trained on; external data must be retrieved and provided.  \n- RAG is a common and effective approach to enhance AI responses by combining model knowledge with up-to-date or proprietary information.  \n- Ensuring the retrieved data is high quality and relevant is critical for obtaining useful AI outputs.  \n- Tools like Azure AI Search simplify the retrieval process by indexing data and exposing APIs for integration with AI applications."
  },
  {
    "section_title": "Azure AI Search",
    "timestamp_range": "00:03:12 \u2013 00:08:24",
    "level": 2,
    "order": 4,
    "content": "### \ud83c\udfa4 [00:03:12 \u2013 00:08:24] Azure AI Search  \n**Timestamp**: 00:03:12 \u2013 00:08:24\n\n**Key Concepts**  \n- Azure AI Search provides an API endpoint to query indexes built over various data sources.  \n- Indexes represent structured references to specific information sources (e.g., blobs, databases).  \n- Supports both lexical (keyword-based) and semantic (meaning-based) search methods.  \n- Semantic search uses vector embeddings to capture the meaning of data and queries.  \n- Reciprocal rank fusion combines lexical and semantic search results for improved relevance.  \n- Semantic re-ranking further refines results based on confidence scoring.  \n- Searches are performed against a single index representing a single information source.  \n- Azure AI Search forms the foundation of a Retrieval-Augmented Generation (RAG) 1.0 stack.\n\n**Definitions**  \n- **Index**: A structured representation of a particular set of information (e.g., a blob container or database) used to enable efficient search.  \n- **Lexical Search**: Keyword-based search matching exact terms or phrases in the indexed data.  \n- **Semantic Search**: Search based on the meaning of words and phrases, using vector embeddings to find conceptually similar information.  \n- **Vector Embeddings**: High-dimensional numerical representations of text chunks that capture semantic meaning.  \n- **Reciprocal Rank Fusion**: A method to combine rankings from multiple search approaches (lexical and semantic) to produce a unified, more relevant result list.  \n- **Semantic Re-rank**: A post-processing step that reorders search results by comparing them again to the query to improve accuracy and confidence.\n\n**Key Facts**  \n- Azure AI Search creates indexes for each distinct data source.  \n- Queries to Azure AI Search are sent via API calls targeting specific indexes.  \n- The system chunks data into blocks to generate vector embeddings for semantic search.  \n- Both lexical and vector-based searches run in parallel, then results are fused and re-ranked.  \n- The final results returned meet a confidence threshold to ensure quality.  \n- Current searches operate on a single index at a time, limiting the scope to one information source per query.\n\n**Examples**  \n- Searching by SKU or product name uses lexical keyword search due to high discrimination ability.  \n- The idiom \u201cit\u2019s raining cats and dogs\u201d illustrates the need for semantic understanding rather than literal keyword matching.\n\n**Key Takeaways \ud83c\udfaf**  \n- Azure AI Search enhances traditional keyword search by integrating semantic search capabilities.  \n- Combining lexical and semantic search results yields higher quality and more relevant answers.  \n- Vector embeddings are crucial for capturing the meaning behind natural language queries and data.  \n- The approach supports Retrieval-Augmented Generation by enriching prompts with relevant, high-quality information from company data.  \n- Each search is limited to a single index, representing one data source, forming the basis of a RAG 1.0 architecture.  \n- Understanding the distinction between information (raw data) and knowledge (contextualized, meaningful data) is important when using Azure AI Search."
  },
  {
    "section_title": "Foundry IQ",
    "timestamp_range": "00:08:24 \u2013 00:09:03",
    "level": 2,
    "order": 5,
    "content": "### \ud83c\udfa4 [00:08:24 \u2013 00:09:03] Foundry IQ  \n**Timestamp**: 00:08:24 \u2013 00:09:03\n\n**Key Concepts**  \n- Foundry IQ is built on Azure AI Search but extends its capabilities.  \n- It provides a true knowledge layer rather than just an information layer.  \n- Moves beyond single-shot Retrieval-Augmented Generation (RAG) to agentic RAG.  \n- Supports multi-hop querying across multiple knowledge sources instead of a single index.\n\n**Definitions**  \n- **Foundry IQ**: An enhanced search capability built on Azure AI Search that enables multi-source, agentic retrieval and knowledge integration rather than simple single-index information retrieval.  \n- **Agentic RAG**: A retrieval approach that allows multi-hop queries and interaction across multiple knowledge sources, as opposed to single-shot RAG which queries only one information source.\n\n**Key Facts**  \n- Traditional RAG 1.0 stacks query a single index representing a single information source.  \n- Foundry IQ enables searching across more than one knowledge source in a single request.\n\n**Examples**  \n- Instead of querying just index one, Foundry IQ can query index one and index two simultaneously to gather applicable information related to real-world entities, relationships, or constraints.\n\n**Key Takeaways \ud83c\udfaf**  \n- Foundry IQ represents a significant evolution from single-source search to a multi-source, knowledge-driven search layer.  \n- It enables more complex, agentic retrieval scenarios that better reflect real-world information needs.  \n- This multi-hop capability allows integration of diverse data sources in one query, improving the relevance and completeness of results."
  },
  {
    "section_title": "Agentic RAG",
    "timestamp_range": "00:09:03 \u2013 00:09:32",
    "level": 2,
    "order": 6,
    "content": "### \ud83c\udfa4 [00:09:03 \u2013 00:09:32] Agentic RAG  \n**Timestamp**: 00:09:03 \u2013 00:09:32\n\n**Key Concepts**  \n- Transition from single-shot RAG to agentic RAG  \n- Agentic RAG supports multi-hop retrieval  \n- Ability to search across multiple knowledge sources simultaneously  \n\n**Definitions**  \n- **Agentic RAG**: An advanced retrieval-augmented generation approach that goes beyond a single query to one index by enabling multi-hop searches across multiple knowledge sources in a single request.\n\n**Key Facts**  \n- Unlike single-shot RAG, which queries one index per request, agentic RAG can query multiple indexes  \n- Enables grouping of different knowledge sources to be searched together  \n\n**Examples**  \n- Searching index one and index two simultaneously to gather applicable information related to a real-world entity, relationship, or constraint  \n\n**Key Takeaways \ud83c\udfaf**  \n- Agentic RAG enhances knowledge retrieval by integrating multiple sources in one query  \n- This approach provides a more comprehensive and connected knowledge layer rather than isolated information retrieval  \n- Useful for scenarios where relevant data spans across different indexes or knowledge bases  \n\n---"
  },
  {
    "section_title": "Multiple knowledge sources",
    "timestamp_range": "00:09:32 \u2013 00:10:18",
    "level": 2,
    "order": 7,
    "content": "### \ud83c\udfa4 [00:09:32 \u2013 00:10:18] Multiple knowledge sources  \n**Timestamp**: 00:09:32 \u2013 00:10:18\n\n**Key Concepts**  \n- Searching across multiple knowledge sources simultaneously (multi-hop retrieval).  \n- Grouping different knowledge sources to perform combined searches.  \n- Adding new types of knowledge sources beyond traditional indexes.  \n\n**Definitions**  \n- **Knowledge sources**: Distinct repositories or indexes of data that can be searched to retrieve relevant information.  \n- **Multi-hop retrieval**: The process of querying multiple knowledge sources or indexes in a single request to gather comprehensive information.  \n\n**Key Facts**  \n- It is possible to search across multiple indexes (e.g., index one and index two) within a single query.  \n- Grouping knowledge sources enables more complete and relevant search results by considering different types of data related to entities, relationships, or constraints.  \n- New knowledge sources can include data indexed from Fabrics OneLake and Fabric IQ.  \n- Fabric IQ represents an ontology of enterprise entities, relationships, and properties, adding semantic meaning to the data.  \n\n**Examples**  \n- Searching both index one and index two to retrieve applicable information related to a real-world entity or relationship.  \n- Including Fabric IQ as a knowledge source to leverage enterprise ontology for more meaningful search results.  \n\n**Key Takeaways \ud83c\udfaf**  \n- Moving beyond single-index search to multi-hop, multi-source retrieval enhances the depth and relevance of information found.  \n- Incorporating semantic models like Fabric IQ enriches the understanding of enterprise data beyond raw information.  \n- The concept of knowledge sources is expanding to include diverse data types and semantic layers, improving the utility of search and retrieval systems."
  },
  {
    "section_title": "New types of knowledge source",
    "timestamp_range": "00:10:18 \u2013 00:11:55",
    "level": 2,
    "order": 8,
    "content": "### \ud83c\udfa4 [00:10:18 \u2013 00:11:55] New types of knowledge source  \n**Timestamp**: 00:10:18 \u2013 00:11:55\n\n**Key Concepts**  \n- Expansion of knowledge sources to include new types beyond traditional data stores  \n- Integration of semantic ontologies to add meaning to enterprise data  \n- Indexing of both local and remote knowledge sources for AI search  \n- Use of semantic indexing to improve relevance of information retrieval  \n\n**Definitions**  \n- **Fabric IQ**: Fabric\u2019s ontology of enterprise entities, relationships, and properties that provides semantic meaning to the data stored in OneLake.  \n- **Knowledge Source**: A data repository or system that can be indexed and searched by AI tools to retrieve relevant information.  \n\n**Key Facts**  \n- Knowledge sources now include data in Fabrics OneLake and Fabric IQ ontology models.  \n- SharePoint sites can be individually indexed as knowledge sources, creating local indexes with embeddings and vectors.  \n- Remote knowledge sources are supported, such as Work IQ or M365 SharePoint, which use Microsoft 365\u2019s semantic index.  \n- Access to M365 semantic index requires a Copilot license.  \n- Additional remote sources include web search powered by Bing and preview access to MCP (a standard for providing knowledge, tools, and prompts to AI applications).  \n\n**Examples**  \n- Indexing a specific SharePoint site to create a local AI search index.  \n- Using M365 SharePoint semantic index to find relevant information based on the user\u2019s token.  \n- Making AI requests to the web via Bing.  \n- Preview capability to point to MCP for enhanced AI knowledge and tooling.  \n\n**Key Takeaways \ud83c\udfaf**  \n- The concept of knowledge sources is broadening to include semantic models and remote indexes, enhancing AI\u2019s understanding and retrieval capabilities.  \n- Fabric IQ adds enterprise semantic context, making data more meaningful and useful.  \n- Both local and remote knowledge sources can be integrated into AI search workflows.  \n- Licensing (e.g., Copilot) may be required to access certain semantic indexes.  \n- The system supports flexible knowledge integration, including web and standardized AI knowledge platforms like MCP."
  },
  {
    "section_title": "Remote knowledge sources",
    "timestamp_range": "00:11:55 \u2013 00:14:22",
    "level": 2,
    "order": 9,
    "content": "### \ud83c\udfa4 [00:11:55 \u2013 00:14:22] Remote knowledge sources  \n**Timestamp**: 00:11:55 \u2013 00:14:22\n\n**Key Concepts**  \n- Remote knowledge sources allow AI applications to access information beyond local indexes.  \n- Integration with Microsoft 365 (M365) semantic index enables retrieval of relevant data based on user tokens.  \n- Use of external web search powered by Bing as a remote knowledge source.  \n- MCP (Microsoft Connected Platform) as a standard protocol to provide additional knowledge, tools, and prompts to AI applications.  \n- Reflection of capabilities by MCP allows AI apps to understand how to use the knowledge source without manual explanation.  \n- Azure AI Search can index data from sources like SharePoint sites or OneLake, respecting permissions.  \n- Different knowledge sources can be combined into knowledge bases within Azure AI Search.  \n\n**Definitions**  \n- **Remote knowledge sources**: External or non-local data repositories or services that an AI application can query to retrieve relevant information.  \n- **M365 semantic index**: A Microsoft 365 service that indexes content semantically to enable relevant search results personalized by user context.  \n- **MCP (Microsoft Connected Platform)**: A standard protocol that exposes knowledge and tools (including search capabilities) to AI applications, allowing them to dynamically understand and use these resources.  \n\n**Key Facts**  \n- Access to M365 semantic index requires a Copilot license.  \n- Web search is powered by Bing and can be used as a remote knowledge source.  \n- MCP is currently in preview and supports exposing search tools as part of its capabilities.  \n- Azure AI Search creates indexes for data from sources like SharePoint or OneLake while retaining permissions.  \n- Using SharePoint in general (not a specific site) defaults to using M365 Work IQ semantic index.  \n- AI apps and users experience no difference whether data is local or from remote knowledge sources.  \n\n**Examples**  \n- Specific SharePoint site as a knowledge source.  \n- General M365 Work IQ / SharePoint semantic index usage.  \n- Web search via Bing as a remote knowledge source.  \n- MCP providing additional knowledge and tools, including search capabilities.  \n- In Microsoft Foundry, a user selects an Azure AI Search resource and creates knowledge bases composed of multiple knowledge sources (e.g., Azure AI Search Index and web).  \n\n**Key Takeaways \ud83c\udfaf**  \n- Remote knowledge sources expand AI app capabilities by integrating diverse data repositories seamlessly.  \n- Licensing (Copilot) and protocols (MCP) are important considerations for accessing certain semantic indexes and tools.  \n- Azure AI Search manages indexing and permissions, ensuring secure and relevant data retrieval.  \n- MCP standardizes how AI apps discover and use external knowledge sources, reducing manual configuration.  \n- Combining multiple knowledge sources into knowledge bases allows flexible and powerful AI-driven search experiences."
  },
  {
    "section_title": "Knowledge bases and use of Azure AI Search resource",
    "timestamp_range": "00:14:22 \u2013 00:15:44",
    "level": 2,
    "order": 10,
    "content": "### \ud83c\udfa4 [00:14:22 \u2013 00:15:44] Knowledge bases and use of Azure AI Search resource  \n**Timestamp**: 00:14:22 \u2013 00:15:44\n\n**Key Concepts**  \n- Azure AI Search resource must be created first before creating knowledge bases.  \n- A knowledge base is a collection of one or more knowledge sources.  \n- Knowledge sources can include Azure AI Search Index, web, SharePoint site indexes, BLOB containers, etc.  \n- Knowledge bases are the primary entities users interact with, sitting above knowledge sources.  \n- Users can add existing knowledge sources or create new ones within a knowledge base.  \n\n**Definitions**  \n- **Azure AI Search resource**: A prerequisite resource in Azure that enables indexing and searching capabilities.  \n- **Knowledge base**: A container or collection of multiple knowledge sources used for search and information retrieval.  \n- **Knowledge source**: An individual data source or index (e.g., Azure AI Search Index, web, SharePoint) that feeds into a knowledge base.  \n\n**Key Facts**  \n- You can have up to 10 knowledge sources within a single knowledge base (subject to change).  \n- Knowledge sources can be reused across multiple knowledge bases.  \n\n**Examples**  \n- Two knowledge bases shown: each contains knowledge sources such as an Azure AI Search Index and a web source.  \n- Knowledge sources can point to:  \n  - Existing Azure AI Search indexes  \n  - BLOB containers (documents are chunked and indexed)  \n  - Web content  \n  - Specific SharePoint site indexes or SharePoint in general  \n\n**Key Takeaways \ud83c\udfaf**  \n- Always create an Azure AI Search resource before setting up knowledge bases.  \n- Think of knowledge bases as the main interface, composed of multiple knowledge sources.  \n- Flexibility to add existing or new knowledge sources allows for diverse data integration.  \n- Knowledge sources can be shared and reused, promoting efficient resource management.  \n- The current limit of 10 knowledge sources per knowledge base may evolve."
  },
  {
    "section_title": "Adding knowledge sources",
    "timestamp_range": "00:15:44 \u2013 00:17:09",
    "level": 2,
    "order": 11,
    "content": "### \ud83c\udfa4 [00:15:44 \u2013 00:17:09] Adding knowledge sources  \n**Timestamp**: 00:15:44 \u2013 00:17:09\n\n**Key Concepts**  \n- A knowledge base consists of multiple knowledge sources.  \n- You can add existing knowledge sources or create new ones.  \n- Knowledge sources can be various types of data repositories or indexes.  \n- Knowledge sources live within Azure AI Search instances.  \n- Different Azure AI Search SKUs support different limits on knowledge bases and knowledge sources.\n\n**Definitions**  \n- **Knowledge base**: A collection of knowledge sources.  \n- **Knowledge source**: A data repository or index that provides content to be included in a knowledge base (e.g., search indexes, blob storage, SharePoint sites).\n\n**Key Facts**  \n- Current example uses 10 knowledge sources, but this number can change.  \n- Knowledge sources can be:  \n  - Existing search indexes  \n  - Blob containers (automatically chunked and indexed)  \n  - Web content  \n  - SharePoint site indexes (creating an Azure AI Search index)  \n  - SharePoint in general (using remote semantic index of M365)  \n  - Fabrics OneLake (retrieves data and creates an index)  \n- MCP (Microsoft Content Platform) integration is in private preview and not available in the demo.  \n- Multiple Azure AI Search resources can be added, each with different knowledge bases.  \n- Azure AI Search SKU limits example:  \n  - Free SKU supports 3 knowledge sources and 3 knowledge bases.  \n  - Basic SKU supports 15 knowledge sources (with some date-related exceptions).  \n  - Higher SKUs (S1, S2, S3, etc.) support progressively more knowledge sources and bases.\n\n**Examples**  \n- Pointing a knowledge source to a blob container for automatic chunking and indexing.  \n- Using a SharePoint site index to create an Azure AI Search index.  \n- Using SharePoint with the remote semantic index of M365.  \n- Using Fabrics OneLake as a knowledge source.\n\n**Key Takeaways \ud83c\udfaf**  \n- Knowledge bases are flexible collections of various knowledge sources.  \n- You can reuse existing knowledge sources across knowledge bases.  \n- Azure AI Search instances host knowledge sources and knowledge bases.  \n- Be aware of SKU limits when planning the number of knowledge sources and knowledge bases.  \n- New types of knowledge sources and integrations (like MCP) are evolving and may become available later."
  },
  {
    "section_title": "SKU limits",
    "timestamp_range": "00:17:09 \u2013 00:17:46",
    "level": 2,
    "order": 12,
    "content": "### \ud83c\udfa4 [00:17:09 \u2013 00:17:46] SKU limits  \n**Timestamp**: 00:17:09 \u2013 00:17:46\n\n**Key Concepts**  \n- Different SKUs (stock keeping units) of Azure AI Search have varying limits on the number of knowledge bases and knowledge sources they support.  \n- Knowledge bases are collections of knowledge sources grouped together to represent useful knowledge about enterprise entities or other topics.  \n- Once a knowledge source is created, it can be reused across multiple knowledge bases.\n\n**Definitions**  \n- **SKU (Stock Keeping Unit)**: A specific version or tier of Azure AI Search service that determines capacity and feature limits.  \n- **Knowledge Source**: An individual data source or repository of information that can be added to Azure AI Search.  \n- **Knowledge Base**: A collection of one or more knowledge sources grouped to represent a coherent set of information.\n\n**Key Facts**  \n- Free SKU supports up to 3 knowledge sources and 3 knowledge bases.  \n- Basic SKU supports up to 15 knowledge sources and knowledge bases (with some historical exceptions based on creation date).  \n- Higher SKUs like S1, S2, S3 support progressively larger numbers of knowledge sources and knowledge bases (exact numbers not specified in this excerpt).  \n\n**Examples**  \n- None explicitly mentioned in this section, but the concept of grouping multiple knowledge sources (e.g., blobs, policies, SharePoint data) into knowledge bases is implied.\n\n**Key Takeaways \ud83c\udfaf**  \n- When planning Azure AI Search deployments, choose the SKU based on the number of knowledge sources and knowledge bases needed.  \n- Knowledge sources are reusable assets that can be combined flexibly into multiple knowledge bases.  \n- Check the official limits page regularly as SKU limits may change over time."
  },
  {
    "section_title": "Collections of knowledge sources",
    "timestamp_range": "00:17:46 \u2013 00:18:49",
    "level": 2,
    "order": 13,
    "content": "### \ud83c\udfa4 [00:17:46 \u2013 00:18:49] Collections of knowledge sources  \n**Timestamp**: 00:17:46 \u2013 00:18:49\n\n**Key Concepts**  \n- A knowledge base is a collection of multiple knowledge sources grouped together.  \n- Knowledge sources can be reused across different knowledge bases.  \n- AI apps or agents interact with knowledge bases rather than individual knowledge sources.  \n- This abstraction simplifies querying multiple disparate data stores.  \n- The system enables richer and more unified search results across all knowledge sources in a knowledge base.\n\n**Definitions**  \n- **Knowledge Source**: An individual repository or data set containing specific knowledge or information.  \n- **Knowledge Base**: A collection of knowledge sources combined to represent useful knowledge about entities or topics relevant to an enterprise or application.\n\n**Key Facts**  \n- Different SKUs support different numbers of knowledge sources (e.g., Basic SKU supports 15 knowledge sources unless created earlier).  \n- Previously, agents had to query multiple separate data stores (e.g., blobs, SharePoint) individually and merge results manually.  \n- Now, AI apps can simply point to a knowledge base to retrieve all relevant information without managing separate queries.\n\n**Examples**  \n- None explicitly mentioned, but implied examples include querying across blobs, policies, and SharePoint documents consolidated into a single knowledge base.\n\n**Key Takeaways \ud83c\udfaf**  \n- Knowledge bases abstract away the complexity of managing multiple knowledge sources.  \n- AI applications benefit by querying a single knowledge base rather than multiple disparate sources.  \n- This approach leads to cleaner, more efficient data retrieval and richer search capabilities.  \n- Reusability of knowledge sources across knowledge bases enhances flexibility and organization.  \n\n---"
  },
  {
    "section_title": "Reasoning effort",
    "timestamp_range": "00:18:49 \u2013 00:22:31",
    "level": 2,
    "order": 14,
    "content": "### \ud83c\udfa4 [00:18:49 \u2013 00:22:31] Reasoning effort  \n**Timestamp**: 00:18:49 \u2013 00:22:31\n\n**Key Concepts**  \n- Reasoning effort levels control how the AI agent queries knowledge sources.  \n- Agentic RAG (Retrieval-Augmented Generation) enables multi-hop reasoning and selective querying.  \n- Different reasoning effort settings: minimal, low, medium, each affecting the agent\u2019s behavior.  \n- Knowledge source descriptions and retrieval instructions guide the agent\u2019s planning and selection.  \n- The agent can break down queries into sub-queries and selectively use knowledge sources based on complexity.  \n\n**Definitions**  \n- **Reasoning Effort**: The level of cognitive work the AI agent applies when deciding how to query knowledge sources, ranging from minimal (simple, broad querying) to medium (planned, selective querying).  \n- **Agentic RAG**: A multi-hop retrieval-augmented generation approach where the agent intelligently plans and reasons about which knowledge sources to query and how to combine information.  \n\n**Key Facts**  \n- Minimal reasoning effort: The agent sends the original query to all knowledge sources without filtering or planning.  \n- Low and medium reasoning effort: The agent analyzes knowledge source descriptions and retrieval instructions to plan queries efficiently.  \n- Minimal effort mode may be disabled (grayed out) depending on the model or output mode used (e.g., web mode forces certain settings).  \n- Quality and clarity of knowledge source descriptions are critical for effective reasoning and source selection.  \n\n**Examples**  \n- A knowledge source labeled \"YouTube\" with the description: \"Transcripts from John Savill's technical training, YouTube channel content.\"  \n- Another knowledge source described as \"search of web information.\"  \n- For simple questions, the agent may select a single knowledge source; for complex questions, it may query multiple sources with sub-queries.  \n\n**Key Takeaways \ud83c\udfaf**  \n- Reasoning effort settings directly influence how the AI agent interacts with knowledge sources\u2014higher effort means smarter, more selective querying.  \n- Providing detailed and accurate descriptions for each knowledge source helps the agent make better decisions during reasoning.  \n- The agent\u2019s ability to break down queries and selectively use sources improves efficiency and relevance of responses.  \n- Understanding and configuring reasoning effort is essential for optimizing AI app performance when using multiple knowledge bases.  \n\n---"
  },
  {
    "section_title": "Importance of good descriptions and instructions",
    "timestamp_range": "00:22:31 \u2013 00:23:51",
    "level": 2,
    "order": 15,
    "content": "### \ud83c\udfa4 [00:22:31 \u2013 00:23:51] Importance of good descriptions and instructions  \n**Timestamp**: 00:22:31 \u2013 00:23:51\n\n**Key Concepts**  \n- Quality descriptions of knowledge sources are crucial for effective selection by the model.  \n- The model uses descriptions to decide which knowledge sources to query based on the question complexity.  \n- Providing clear instructions guides the model on prioritizing knowledge sources during query planning.  \n- Agentic multi-hop querying involves creating a chain of thought or query plan to select and execute queries across sources.  \n\n**Definitions**  \n- **Knowledge Source (KS)**: A repository or dataset (e.g., YouTube transcripts, web search) that the model can query for information.  \n- **Agentic Multi-hop**: A process where the model plans multiple queries across different knowledge sources in a sequence to answer complex questions.  \n\n**Key Facts**  \n- Descriptions help the model understand the content and relevance of each knowledge source.  \n- Instructions can specify priority, e.g., always use a technical YouTube knowledge source first for technical questions, then fallback to web search if needed.  \n- The model\u2019s decision-making involves a train of thought to select the most appropriate knowledge sources.  \n\n**Examples**  \n- YouTube knowledge source described as: \"Transcripts from John Savill's technical training, YouTube channel content.\"  \n- Instruction example: For technical items, prioritize the YouTube knowledge source first; if no answer found, then use the web search knowledge source.  \n\n**Key Takeaways \ud83c\udfaf**  \n- Always provide clear, quality descriptions for each knowledge source to enable accurate source selection by the model.  \n- Use explicit instructions to guide the model\u2019s query strategy and source prioritization.  \n- Good descriptions and instructions improve the model\u2019s ability to create an effective multi-hop query plan and solve complex problems efficiently.  \n- This approach supports a self-reflective process where the model can reassess and refine its queries if initial attempts do not yield satisfactory answers.  \n\n---"
  },
  {
    "section_title": "Self-reflection",
    "timestamp_range": "00:23:51 \u2013 00:25:39",
    "level": 2,
    "order": 16,
    "content": "### \ud83c\udfa4 [00:23:51 \u2013 00:25:39] Self-reflection  \n**Timestamp**: 00:23:51 \u2013 00:25:39\n\n**Key Concepts**  \n- Multi-hop query planning and execution for problem solving  \n- Self-reflection step to evaluate if the query results have sufficiently answered the question  \n- Different effort levels (minimal, low, medium) affect query planning, source selection, and self-reflection  \n- Higher effort levels enable additional retrieval passes and richer response classification  \n\n**Definitions**  \n- **Self-reflection**: A process where, after executing a query plan and retrieving results, the system reviews the results against the original goal to determine if the problem is solved. If not, it can perform a follow-up query to improve the answer.  \n- **Effort levels**: Settings that control the depth of query planning and reasoning:  \n  - *Minimal*: No source selection or planning; searches all sources simply and quickly.  \n  - *Low/Medium*: Includes source selection, query planning, and self-reflection with additional retrieval and classification steps.  \n\n**Key Facts**  \n- Medium effort includes a self-reflection step that can trigger a second retrieval pass if the initial results are insufficient.  \n- Higher effort levels consume more tokens, increase latency, and cost more but yield higher quality responses.  \n- Minimal effort skips source selection and planning, does not use web as a knowledge source grounding.  \n- Low and medium efforts perform source selection, query planning, and self-reflection.  \n- Reflective retrieval and classification add richer capabilities to responses at low and medium levels.  \n\n**Examples**  \n- None explicitly mentioned beyond the description of how minimal vs. medium effort modes operate.  \n\n**Key Takeaways \ud83c\udfaf**  \n- Choosing the effort level balances cost, speed, and answer quality.  \n- Self-reflection enables iterative improvement of answers by reassessing query results and performing follow-ups if needed.  \n- Minimal effort is fast and basic but less precise; medium effort is more thorough with additional reasoning and retrieval steps.  \n- Source selection and query planning are critical for higher quality responses and are enabled starting at low effort level.  \n- Understanding these modes helps optimize the use of knowledge sources and response quality in AI agents.  \n\n---"
  },
  {
    "section_title": "Output modes",
    "timestamp_range": "00:25:39 \u2013 00:28:31",
    "level": 2,
    "order": 17,
    "content": "### \ud83c\udfa4 [00:25:39 \u2013 00:28:31] Output modes  \n**Timestamp**: 00:25:39 \u2013 00:28:31\n\n**Key Concepts**  \n- Output modes determine how data is returned from knowledge base queries.  \n- Two main output modes: **extractive data** and **answer synthesis**.  \n- Answer synthesis is only available when using low and medium reasoning levels and requires the knowledge source to exclude web content.  \n- Extractive data mode returns relevant data snippets for the generative model to reason over and produce answers.  \n- Answer synthesis mode performs more advanced reasoning and synthesis of answers directly.  \n\n**Definitions**  \n- **Extractive data**: An output mode where the system returns relevant data excerpts from the knowledge base, leaving the generative model to reason and formulate the final answer.  \n- **Answer synthesis**: An output mode where the system synthesizes and generates answers directly from the knowledge base content, involving more reasoning steps.  \n\n**Key Facts**  \n- When the knowledge source includes web content, only answer synthesis mode is allowed; minimal reasoning cannot be selected.  \n- Answer synthesis mode is disabled (grayed out) if web is included as a knowledge source.  \n- Extractive data mode is the default and compatible with all knowledge sources.  \n- The API request changes from querying indexes to querying knowledge bases or knowledge sources when using these output modes.  \n\n**Examples**  \n- Using extractive data mode with a knowledge base to answer the question: \"What is ExpressRoute?\"  \n- The system queries the knowledge base, retrieves relevant data, and then the generative model reasons over this data to produce the answer.  \n\n**Key Takeaways \ud83c\udfaf**  \n- Choose extractive data mode if you want raw relevant data returned for your own reasoning or processing.  \n- Use answer synthesis mode for more advanced, integrated answer generation but only when web content is not part of the knowledge sources.  \n- The choice of output mode affects API behavior and capabilities such as reasoning level and source selection.  \n- Understanding output modes is crucial for configuring knowledge base queries effectively in Azure AI search scenarios."
  },
  {
    "section_title": "Seeing the output modes in action",
    "timestamp_range": "00:28:31 \u2013 00:33:11",
    "level": 2,
    "order": 18,
    "content": "### \ud83c\udfa4 [00:28:31 \u2013 00:33:11] Seeing the output modes in action  \n**Timestamp**: 00:28:31 \u2013 00:33:11\n\n**Key Concepts**  \n- Two main output modes when querying knowledge bases with generative AI:  \n  1. **Extractive data mode** \u2013 returns raw chunks of relevant data/documents from the knowledge base.  \n  2. **Answer synthesis mode** \u2013 generates a natural language answer synthesized from multiple knowledge sources.  \n- Choice of output mode depends on the application\u2019s needs (e.g., rich agent vs. simple chat app).  \n- Debugging and inspecting returned data helps understand what the AI is doing behind the scenes.\n\n**Definitions**  \n- **Extractive data mode**: A mode where the AI returns a set of relevant documents or data chunks retrieved from the knowledge base without further processing or summarization.  \n- **Answer synthesis mode**: A mode where the AI generates a coherent, natural language answer by synthesizing information from various knowledge sources, including indexes and the web.\n\n**Key Facts**  \n- In extractive mode, the system retrieved 11 documents as chunks from the knowledge base.  \n- Answer synthesis mode can combine multiple sources such as an index and the web.  \n- Answer synthesis typically takes longer to process than extractive mode.  \n- The synthesized answer includes references to the sources used.  \n- Different knowledge sources (e.g., BLOB, OneLake, web) influence the mode used; web sources require answer synthesis.  \n- Medium reasoning effort can be configured for answer synthesis.\n\n**Examples**  \n- Asking \u201cWhat is ExpressRoute?\u201d in extractive mode returns raw document chunks from the knowledge base.  \n- Asking the same question in answer synthesis mode returns a fully formed natural language answer along with source references.  \n- A demonstration app uses three knowledge sources (BLOB, OneLake, web) and performs answer synthesis with medium reasoning effort.\n\n**Key Takeaways \ud83c\udfaf**  \n- Use extractive data mode when building rich agents that need raw data to reason over themselves.  \n- Use answer synthesis mode for simpler chat applications that want a direct, natural language answer without additional processing.  \n- Inspecting debug output is valuable to understand the AI\u2019s behavior and the data it uses.  \n- The choice of output mode should align with the intended user experience and application complexity.  \n- Knowledge sources impact the mode: web-based sources necessitate answer synthesis.  \n- Answer synthesis provides a more user-friendly, complete answer but requires more processing time.  \n\n---"
  },
  {
    "section_title": "Peeking inside its thinking",
    "timestamp_range": "00:33:11 \u2013 00:34:37",
    "level": 2,
    "order": 19,
    "content": "### \ud83c\udfa4 [00:33:11 \u2013 00:34:37] Peeking inside its thinking  \n**Timestamp**: 00:33:11 \u2013 00:34:37\n\n**Key Concepts**  \n- Using different knowledge sources (BLOB, OneLake, web) to answer queries  \n- Answer synthesis vs. extractive data retrieval based on use case  \n- Visibility into the internal reasoning and processing steps of the AI system  \n- Iterative planning and multiple retrieval calls in generating answers  \n- Transparency in AI operations to understand how answers are formed  \n\n**Definitions**  \n- **Answer Synthesis**: The process where the AI generates a full, composed answer by integrating information from multiple sources, often involving reasoning and extrapolation.  \n- **Extractive Data Option**: A retrieval mode where the AI returns only the most relevant data or information without generating a synthesized answer.  \n\n**Key Facts**  \n- The example system uses three knowledge sources: BLOB storage, OneLake, and the web  \n- When using the web as a source, answer synthesis is required  \n- The system performed 2 iterations (planning cycles)  \n- It executed 11 different activities and 60 retrieval calls during the process  \n- Detailed metrics available include elapsed time, planning tokens, query planning steps, and references used  \n\n**Examples**  \n- A demonstration from the product group showing the AI querying a knowledge base with multiple sources and revealing behind-the-scenes details of its reasoning process  \n- The system\u2019s output includes the number of iterations, activities, retrieval calls, and the step-by-step query planning and execution  \n\n**Key Takeaways \ud83c\udfaf**  \n- Foundry IQ enables querying a unified knowledge base that intelligently integrates multiple data sources  \n- Users can choose between getting raw relevant data or a fully synthesized answer depending on their needs  \n- The system provides transparency by exposing its internal reasoning steps, helping users understand how answers are derived  \n- This visibility into the AI\u2019s \u201cthinking\u201d process enhances trust and debugging capabilities for complex queries  \n- The ultimate goal is to deliver high-quality inferencing and flexible answer generation for agents or applications using the knowledge base  \n\n---"
  },
  {
    "section_title": "Summary",
    "timestamp_range": "00:34:37 \u2013 00:35:15",
    "level": 2,
    "order": 20,
    "content": "### \ud83c\udfa4 [00:34:37 \u2013 00:35:15] Summary  \n**Timestamp**: 00:34:37 \u2013 00:35:15\n\n**Key Concepts**  \n- Foundry IQ enables querying a unified knowledge base.  \n- The knowledge base acts as a single queryable entity that intelligently utilizes multiple knowledge sources.  \n- Knowledge sources can be local or remote.  \n- The system returns the most relevant data to provide high-quality inferencing or synthesize a complete answer.  \n\n**Definitions**  \n- **Knowledge Base**: A unified repository that integrates various knowledge sources (local and remote) to respond intelligently to queries.  \n\n**Key Facts**  \n- Foundry IQ supports querying across multiple knowledge sources seamlessly.  \n- The goal is to deliver high-quality inferencing or synthesized answers from the aggregated data.  \n\n**Examples**  \n- None mentioned explicitly in this segment, but the concept of using local and remote sources to retrieve relevant data was highlighted.  \n\n**Key Takeaways \ud83c\udfaf**  \n- The core value of Foundry IQ lies in its ability to unify diverse knowledge sources into a single queryable interface.  \n- This approach enhances the quality of answers by leveraging the most relevant data available.  \n- The system supports both inferencing (drawing conclusions) and full answer synthesis."
  },
  {
    "section_title": "How the IQs work together",
    "timestamp_range": "00:35:15 \u2013 00:37:43",
    "level": 2,
    "order": 21,
    "content": "### \ud83c\udfa4 [00:35:15 \u2013 00:37:43] How the IQs work together  \n**Timestamp**: 00:35:15 \u2013 00:37:43\n\n**Key Concepts**  \n- Progression from raw data \u2192 information \u2192 knowledge \u2192 wisdom  \n- Knowledge bases as consolidated, contextualized, and interpreted data enabling reasoning  \n- Different IQs (Foundry IQ, Fabric IQ, Work IQ) provide specialized knowledge domains  \n- AI apps and agents leverage combined knowledge from multiple IQs to generate wisdom and make informed decisions  \n- Integration of multiple IQs provides comprehensive understanding across user context, system operations, and organizational information  \n\n**Definitions**  \n- **Knowledge**: Data that has been consolidated, grouped, interpreted, and contextualized to enable reasoning.  \n- **Wisdom**: The ability to make judgments and apply knowledge effectively.  \n- **Foundry IQ**: Provides knowledge bases focused on organizational information.  \n- **Fabric IQ**: Works with data in OneLake, mapping it to enterprise entities via ontologies to provide system operational knowledge.  \n- **Work IQ**: Enhances data from M365 with memory, customization, and richer inferencing to provide user context knowledge.  \n\n**Key Facts**  \n- Knowledge bases enable high-quality inferencing and synthesis of answers by AI agents or apps.  \n- Foundry IQ, Fabric IQ, and Work IQ each specialize in different data domains but collectively support AI applications.  \n- AI apps and agents use these IQs together to access quality knowledge and provide wisdom for decision-making.  \n\n**Examples**  \n- Fabric IQ shortcuts to OneLake using ontologies to map data to real enterprise entities.  \n- Work IQ adds memory and customization to M365 data for richer inferencing.  \n- Combined use: Work IQ gives user context, Fabric IQ provides system operational knowledge, Foundry IQ offers organizational knowledge\u2014together enabling AI to answer any question effectively.  \n\n**Key Takeaways \ud83c\udfaf**  \n- The value lies in combining multiple IQ knowledge bases to provide comprehensive, high-quality knowledge for AI applications.  \n- Moving beyond raw data to knowledge and wisdom is essential for effective AI reasoning and decision-making.  \n- Each IQ complements the others by focusing on different aspects of enterprise data and context.  \n- AI apps and agents become more capable and accurate when leveraging integrated knowledge from all IQs.  \n\n---"
  },
  {
    "section_title": "Close",
    "timestamp_range": "00:37:43 \u2013 unknown",
    "level": 2,
    "order": 22,
    "content": "### \ud83c\udfa4 [00:37:43 \u2013 ??:??:??] Close  \n**Timestamp**: 00:37:43 \u2013 unknown\n\n**Key Concepts**  \n- Integration of multiple IQs (Work IQ, Fabric IQ, Founder IQ) to provide comprehensive knowledge for AI apps and agents  \n- The combined use of these IQs enables AI to understand user context, system operations, and organizational information  \n\n**Definitions**  \n- **Work IQ**: Provides understanding of the full user context  \n- **Fabric IQ**: Provides full system operational knowledge  \n- **Founder IQ**: Provides comprehensive knowledge about organizational information  \n\n**Key Facts**  \n- Using these IQs together allows AI applications and agents to answer any question effectively  \n\n**Examples**  \n- None mentioned  \n\n**Key Takeaways \ud83c\udfaf**  \n- Combining different knowledge domains (user, system, organization) is essential for AI to perform well  \n- The goal is to equip AI apps and agents with quality, comprehensive knowledge to enable correct actions  \n- The video series aims to provide useful insights and will continue in future installments"
  }
]