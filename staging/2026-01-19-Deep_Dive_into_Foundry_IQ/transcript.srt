1
00:00:00,480 --> 00:00:01,040
Hi, everyone.

2
00:00:01,360 --> 00:00:04,400
In this video, I want to dive into Foundry IQ.

3
00:00:04,640 --> 00:00:14,960
This, along with Fabric IQ and Work IQ, form that knowledge foundation for our AI apps and agents that enables us to unlock a lot of powerful capabilities.

4
00:00:15,760 --> 00:00:28,960
Now, our need for any of these additional sets of knowledge comes from the fact that our generative models are trained on a very large but very finite corpus of information.

5
00:00:29,680 --> 00:00:33,120
we think that we have this body of knowledge.

6
00:00:33,120 --> 00:00:35,960
So this is our corpus of training data.

7
00:00:37,040 --> 00:00:42,320
And this comes from books and websites and transcripts.

8
00:00:43,280 --> 00:00:46,800
But it is a finite size and it has a certain cutoff date.

9
00:00:47,160 --> 00:00:49,600
And it won't include anything non-public.

10
00:00:50,400 --> 00:00:58,000
So then this is used to train our generative model.

11
00:00:58,640 --> 00:01:22,640
So all of those neurons that make up the model get its different weights and biases modified based on the strength of those various connections, which then once it's done all that training, I can pass new prompts and then it can predict the next most probable token and the next and the next and it generates us these fantastic responses.

12
00:01:24,320 --> 00:01:27,360
But if I need it to be able to utilize

13
00:01:27,840 --> 00:01:39,440
and information that it was not trained on, it's not part of that pre-trained knowledge that makes up the model, well, I have to be able to give it that information.

14
00:01:39,760 --> 00:01:41,600
I have to provide it as part of the request.

15
00:01:42,160 --> 00:01:50,480
So the way that works is whatever our little AI application is, if there is some prompt,

16
00:01:51,440 --> 00:01:59,480
that I want to give the model, but I need that additional data, what we actually do is we have some knowledge of information.

17
00:01:59,480 --> 00:02:03,520
Maybe it's our internal company set of database, whatever that may be.

18
00:02:03,760 --> 00:02:06,560
So this is all of our information.

19
00:02:07,040 --> 00:02:13,240
The application makes a request to it first, and then it sends back the relevant info.

20
00:02:14,160 --> 00:02:18,000
Then that info gets added to the prompt.

21
00:02:18,640 --> 00:02:24,280
And now the model can take the request and the applicable information and generate that response.

22
00:02:24,280 --> 00:02:35,080
So this is known as retrieving additional information to augment the ability for it to generate information or RAG.

23
00:02:35,280 --> 00:02:36,840
So it's a super common term.

24
00:02:37,280 --> 00:02:38,080
We're used to that.

25
00:02:38,600 --> 00:02:41,840
And the quality of this additional information, we send it

26
00:02:42,720 --> 00:02:45,840
is going to massively impact the quality of the response we get back.

27
00:02:45,840 --> 00:02:47,200
Garbage in would be garbage out.

28
00:02:47,200 --> 00:02:55,680
So we want to make sure we get the very highest quality and applicability of data that we send to the model so we get fantastic outputs.

29
00:02:56,000 --> 00:02:57,520
So that's the fundamental issue.

30
00:02:57,520 --> 00:02:59,560
The model is trained on a certain body of data.

31
00:02:59,560 --> 00:03:01,960
It doesn't know about our company's data.

32
00:03:01,960 --> 00:03:03,120
It has cutoff dates.

33
00:03:03,360 --> 00:03:07,280
So for it to be really useful, we have to give it more info.

34
00:03:08,240 --> 00:03:11,520
Now I drew this idea of sending a question and getting information back.

35
00:03:12,400 --> 00:03:17,360
This is where Azure AI Search comes in for many of our different types of data.

36
00:03:18,000 --> 00:03:32,090
So the whole point here is what we get is with Azure AI Search, it provides that ability.

37
00:03:32,480 --> 00:03:34,320
to expose an endpoint.

38
00:03:34,640 --> 00:03:36,480
So remember, we're used to talking to these things.

39
00:03:36,480 --> 00:03:38,400
It's going to expose an API.

40
00:03:38,880 --> 00:03:42,240
In this case, it's going to create a number of indexes.

41
00:03:42,240 --> 00:03:43,400
We're going to talk to an index.

42
00:03:43,400 --> 00:03:49,280
So the app would now, for this question, would actually send it this way to the API for Azure AI search.

43
00:03:50,320 --> 00:03:56,960
And it sends it, hey, the query it's trying to find, an Azure AI search.

44
00:03:57,440 --> 00:03:59,880
We'll do that, search for it.

45
00:03:59,880 --> 00:04:05,440
will find the best, highest quality data, send it back, which it can then add to the prompt.

46
00:04:05,600 --> 00:04:09,120
So that's the benefit we get from Azure AI Search.

47
00:04:10,400 --> 00:04:20,480
But it's how it finds that information that is so powerful because obviously we, underneath everything, we have some amount of knowledge.

48
00:04:21,760 --> 00:04:22,800
Well, it's not even knowledge.

49
00:04:22,840 --> 00:04:23,720
That's actually a poor word.

50
00:04:23,720 --> 00:04:24,480
I'm going to change that.

51
00:04:24,720 --> 00:04:26,880
We have some information.

52
00:04:28,000 --> 00:04:30,800
And there's a distinction between information and knowledge.

53
00:04:31,360 --> 00:04:33,600
So we have a set of different information.

54
00:04:33,760 --> 00:04:39,200
This could be in blobs, could be in databases, whatever that may be.

55
00:04:42,400 --> 00:04:46,800
What Azure AI Search is going to do is a number of different ways.

56
00:04:47,200 --> 00:04:49,600
So firstly, it's going to create an index.

57
00:04:50,080 --> 00:04:56,640
So the whole point here is it creates a certain index for a particular set of information, a particular BLOB.

58
00:04:57,920 --> 00:05:00,000
container, a particular database.

59
00:05:00,560 --> 00:05:02,480
So in this case, maybe we create index one.

60
00:05:03,680 --> 00:05:11,760
So I create index one, which corresponds to a single source of information.

61
00:05:12,400 --> 00:05:23,680
Now, sometimes when we search, we are searching on maybe a SKU, a name of a product that has a very high discrimination ability against other types of data.

62
00:05:24,480 --> 00:05:26,880
I'm just basically searching A lexical search.

63
00:05:26,880 --> 00:05:28,720
I'm doing a keyword search to match on that.

64
00:05:29,360 --> 00:05:33,040
So when we have these indexes in Azure AI Search, it has that.

65
00:05:33,120 --> 00:05:33,440
Great.

66
00:05:33,760 --> 00:05:39,520
It has that basic keyword lexical index, and it does a lexical search.

67
00:05:40,160 --> 00:05:47,440
However, when we start looking at these generative models and how we interact with them,

68
00:05:48,480 --> 00:05:59,440
We normally use natural language, and the problem of natural language is many words can mean the same thing, and one word can mean many different things.

69
00:06:00,080 --> 00:06:04,560
Our language is full of idioms where what we say should not be taken literally.

70
00:06:05,040 --> 00:06:06,480
It's raining cats and dogs.

71
00:06:06,480 --> 00:06:14,720
If a computer took that literally, it would be taking automated actions to call a lot of vets to your location because it's assuming there's a lot of injured animals on the ground.

72
00:06:15,200 --> 00:06:16,800
We know it just means it's raining A lot.

73
00:06:17,520 --> 00:06:29,680
So what we have to do when we think about natural language, we have to understand the semantic meaning of information and then find the closest set of information based on what we're looking for.

74
00:06:29,680 --> 00:06:33,440
Now the way we do this is we create vectors.

75
00:06:34,080 --> 00:06:38,800
So in addition to this lexical, we have an idea of a vector index.

76
00:06:40,440 --> 00:06:43,840
And what we have to enable this is whatever that source is,

77
00:06:46,080 --> 00:06:58,280
We actually chunk it up into particular blocks, and then we use an embedding model to create this very high dimensional vector that represents the semantic meaning of the data.

78
00:06:58,280 --> 00:07:07,680
And then when we're searching, we create another embedding, a high dimensional vector of what we're looking for, and we find the ones that are closest, because that means they have similar semantic meaning.

79
00:07:09,760 --> 00:07:14,800
So then what Azure AI Search will do is it will run the search against both of those,

80
00:07:17,880 --> 00:07:21,600
And then it does something called reciprocal rank fusion.

81
00:07:23,400 --> 00:07:25,680
Basically a fancy term for mush them together.

82
00:07:27,240 --> 00:07:30,640
Things that rank highly on both of them put overall near the top.

83
00:07:30,960 --> 00:07:36,960
But then it does another semantical re-rank

84
00:07:38,400 --> 00:07:50,560
So that list of results, it then compares again compared to what the search request was, reorders them, scores them, and returns the ones that give us an answer above a certain confidence level.

85
00:07:51,600 --> 00:07:54,640
So this is fantastic capability.

86
00:07:55,200 --> 00:08:07,680
The net result is I get fantastic results, but all of the searches we make are always against a single index, which is against a single

87
00:08:09,440 --> 00:08:10,560
information source.

88
00:08:11,280 --> 00:08:17,040
We really can think about this as our RAG 1.0 stack.

89
00:08:17,200 --> 00:08:19,440
So that's how often we'll think about this.

90
00:08:21,440 --> 00:08:25,600
Okay, so now we're going to think about Foundry IQ.

91
00:08:26,480 --> 00:08:29,040
So if we now take, it's still Azure AI search.

92
00:08:29,040 --> 00:08:31,840
Now this is built on Azure AI search too.

93
00:08:32,640 --> 00:08:35,480
But what we're now going to have, use orange.

94
00:08:35,480 --> 00:08:36,960
I always use orange for the new thing.

95
00:08:37,880 --> 00:08:39,680
So now we're going to have Foundry IQ.

96
00:08:39,680 --> 00:08:51,210
Now the big deal here is it's going to provide a true knowledge layer, not just information.

97
00:08:51,690 --> 00:08:53,930
It moves from a single shot RAG.

98
00:08:53,930 --> 00:08:56,730
Remember, we make a request.

99
00:08:56,730 --> 00:09:01,290
It can look at an index, which represents an information source.

100
00:09:02,250 --> 00:09:06,250
With Foundry IQ, we move to agentic RAG.

101
00:09:07,450 --> 00:09:11,210
So now it's not just this single

102
00:09:13,280 --> 00:09:16,160
shop, we can also think of it as a multi-hop.

103
00:09:20,800 --> 00:09:24,560
I can now think about searching across more than one knowledge source.

104
00:09:25,120 --> 00:09:37,280
So for example, in a single request at a very basic, basic level, what I could now do is, yes, he could still go and look at index one, but also we had index two.

105
00:09:39,680 --> 00:09:54,400
it had some other type of data that when I think about a certain real-world entity or relationship or constraint, this has applicable information as well that needs to be considered.

106
00:09:54,960 --> 00:10:01,840
So I can now group particular sets of knowledge sourced together and search across all of them.

107
00:10:02,960 --> 00:10:09,200
And so maybe that's a key term of what we're going to do here is as I start thinking about this, what we're adding

108
00:10:10,440 --> 00:10:13,920
are knowledge sources.

109
00:10:17,200 --> 00:10:20,880
But it's also adding additional types of knowledge source.

110
00:10:21,440 --> 00:10:31,200
So for example, these indexes it creates, it can also now index data in Fabrics OneLake, Fabric IQ.

111
00:10:31,200 --> 00:10:37,760
So remember, Fabric IQ is Fabric's own ontology of the enterprise entities, relationships, properties,

112
00:10:38,080 --> 00:10:40,800
that gives semantic meaning of the information.

113
00:10:40,800 --> 00:10:48,480
So rather than it just being information in OneLake, it now actually represents what does it mean to the true enterprise?

114
00:10:48,720 --> 00:10:50,800
So it becomes a lot more useful.

115
00:10:51,360 --> 00:10:58,640
So these knowledge sources can now include Fabric and Fabric IQ, i.e.

116
00:10:58,640 --> 00:11:03,120
data in OneLake and that ontology, those models.

117
00:11:04,240 --> 00:11:05,200
It can also

118
00:11:06,720 --> 00:11:12,600
Include a specific SharePoint site, so I can point it to a particular SharePoint site.

119
00:11:14,000 --> 00:11:36,000
Now, with these, it is still going to go ahead and it's going to index it, so it's going to go ahead and again take the data, bring it into an index, create the vectors, create the embeddings, so it's now another index that is local to.

120
00:11:36,400 --> 00:11:37,600
Azure AI Search.

121
00:11:39,280 --> 00:11:44,880
But additionally, it supports the idea of remote knowledge sources.

122
00:11:45,760 --> 00:11:51,600
So here I can think about this was a specific SharePoint site.

123
00:11:52,880 --> 00:12:04,480
But I can also have the idea of actually just work IQ slash sort of M365 SharePoint.

124
00:12:07,160 --> 00:12:21,200
Now, what this is now going to do is use M365's own semantic index to find the most relevant information based on the token, the user who is making the request.

125
00:12:21,600 --> 00:12:27,520
Now, I would need a copilot license to be able to utilize that to get that semantic index access.

126
00:12:28,880 --> 00:12:34,800
I can also make a request to the web, which is powered by Bing.

127
00:12:35,960 --> 00:12:39,680
And also, I think in preview, I can point to MCP.

128
00:12:39,880 --> 00:12:48,880
So remember, MCP is about a standard way to provide additional knowledge and tools, maybe even prompts, to my AI application.

129
00:12:49,160 --> 00:12:55,400
And it reflects its capabilities to me, so I don't have to try and explain it to the app itself on how to use it.

130
00:12:55,400 --> 00:12:56,640
It's a standard protocol.

131
00:12:57,040 --> 00:13:04,000
So what I would probably expect here is if I was going to use MCP, it probably has some kind of search tool

132
00:13:04,800 --> 00:13:08,480
that it exposes as part of that reflection of its capabilities.

133
00:13:08,880 --> 00:13:16,720
So Foundry IQ, Azure AI search behind the scenes, would be able to call that to get the response.

134
00:13:16,960 --> 00:13:22,960
So these are remote knowledge sources.

135
00:13:23,840 --> 00:13:25,840
This makes 0 difference to the AI app.

136
00:13:25,840 --> 00:13:27,760
It makes 0 difference to the user.

137
00:13:28,160 --> 00:13:30,880
It's just for your information when you think about this.

138
00:13:32,000 --> 00:13:42,520
If it's regular BLOB and information or data from OneLake or SharePoint site, it's creating an index within Azure AI Search.

139
00:13:42,520 --> 00:13:48,960
And again, even when it does this stuff on a SharePoint site, it will still try and retain as much information and permissions as it can.

140
00:13:49,680 --> 00:13:58,400
If I add these, like just SharePoint in general, not a particular site, then it's just going to use MS365 Work IQ's semantic index.

141
00:13:58,800 --> 00:14:00,960
If it's going to search the web, it goes and searches the web.

142
00:14:00,960 --> 00:14:02,560
It's not going to index the web itself.

143
00:14:03,200 --> 00:14:05,360
MCP will call that search tool.

144
00:14:05,920 --> 00:14:11,120
So we have those capabilities as part of what we're going to use.

145
00:14:11,600 --> 00:14:18,560
And if we jump over for a second, so here I'm in Microsoft Foundry.

146
00:14:18,560 --> 00:14:23,440
I'm looking at the new experience right here.

147
00:14:25,040 --> 00:14:27,440
And the first thing to pay attention to is

148
00:14:28,000 --> 00:14:32,000
You select a particular Azure AI search resource first.

149
00:14:32,000 --> 00:14:33,200
So this has to exist.

150
00:14:33,600 --> 00:14:40,960
I have to have created an Azure AI search resource and then I create one or more knowledge bases.

151
00:14:40,960 --> 00:14:45,080
So you can see I have two different knowledge bases here.

152
00:14:46,240 --> 00:14:55,600
If I select one of them, what you'll see is it comprises of a certain number of knowledge sources.

153
00:14:55,600 --> 00:14:56,160
I have two.

154
00:14:57,000 --> 00:15:00,720
One is the Azure AI Search Index, and one of them is web.

155
00:15:01,600 --> 00:15:02,800
So I guess that's an important point.

156
00:15:04,240 --> 00:15:12,640
When I think about, okay, knowledge sources, realize what sits above that is a knowledge base.

157
00:15:13,280 --> 00:15:17,960
So I have a knowledge base, which is actually what I really interact with.

158
00:15:20,800 --> 00:15:25,040
So a knowledge base is just a collection

159
00:15:28,000 --> 00:15:29,840
of knowledge sources.

160
00:15:30,160 --> 00:15:34,720
And today you have N number of knowledge sources in a knowledge base.

161
00:15:34,720 --> 00:15:35,520
It's 10.

162
00:15:36,280 --> 00:15:38,960
That could absolutely change in the future.

163
00:15:40,440 --> 00:15:52,880
If we jump back again, but what I can see here is I can add existing knowledge sources that may have already created that maybe I'm using in another knowledge base, or I can go and create a new

164
00:15:54,240 --> 00:15:55,080
type of knowledge source.

165
00:15:55,080 --> 00:15:56,240
And this is where you can see it.

166
00:15:57,040 --> 00:16:00,560
So I could use an existing search index I have.

167
00:16:01,120 --> 00:16:03,080
I could just point it at a BLOB container.

168
00:16:03,080 --> 00:16:05,520
It will go and grab them, chunk them, index them.

169
00:16:06,000 --> 00:16:07,280
I can point it to the web.

170
00:16:08,400 --> 00:16:09,400
I could point it.

171
00:16:09,400 --> 00:16:14,840
So here the point is a particular SharePoint site index.

172
00:16:14,840 --> 00:16:17,440
So it will actually create an index in Azure AI search.

173
00:16:17,520 --> 00:16:21,280
Or I can just point it at SharePoint in general.

174
00:16:21,440 --> 00:16:23,280
This is just saying use SharePoint.

175
00:16:23,920 --> 00:16:28,240
And then it's going to use the remote semantic index of M365.

176
00:16:29,040 --> 00:16:35,520
I can also point it to Fabrics OneLake where it can go and retrieve and then again it will go and create the index.

177
00:16:35,760 --> 00:16:38,640
The ability to use MCP is in private preview today.

178
00:16:38,880 --> 00:16:42,800
I've not got that signed up in this particular one, which is why I can't see it.

179
00:16:44,880 --> 00:16:49,360
But they're all the different types of knowledge source you can actually add.

180
00:16:51,760 --> 00:17:00,080
You saw already they live in a particular instance of Azure AI search, so I can add multiple Azure AI search resources with different sets of knowledge bases in it.

181
00:17:00,960 --> 00:17:09,120
Different SKUs of Azure AI search support different total numbers of knowledge bases and different numbers of knowledge sources.

182
00:17:09,760 --> 00:17:19,440
If I jump over and look at the limits page, and again, you'd want to check this, but today you can see the free version sports only three.

183
00:17:21,040 --> 00:17:22,720
sources and three knowledge bases.

184
00:17:23,280 --> 00:17:34,480
Basic is 15, unless you created it earlier on, it talks about the date, which goes to be 5, then the S1, S2, S3, et cetera, et cetera.

185
00:17:34,480 --> 00:17:38,000
So the different SKUs support a different number of them.

186
00:17:43,380 --> 00:17:48,500
Once I've created the knowledge source, again, I could use it in other

187
00:17:48,800 --> 00:17:49,680
knowledge bases.

188
00:17:49,680 --> 00:18:06,400
It's literally a knowledge base is just a collection of knowledge sources that I'm collecting together because together they represent some useful knowledge about entities of the enterprise or some other type of thing.

189
00:18:06,600 --> 00:18:09,440
And then my AI app or agent doesn't need to care.

190
00:18:09,760 --> 00:18:13,680
It can just point to the knowledge bases I've created and do useful things.

191
00:18:14,400 --> 00:18:16,400
Previously, it was very complicated for agents.

192
00:18:16,800 --> 00:18:26,160
If maybe I needed information from three different blobs, maybe different types of policies and information, then there was maybe something in a SharePoint.

193
00:18:26,800 --> 00:18:35,280
I as writing the agent or the app would have to go and manage and do separate queries and then try and bring them together, it would get very messy.

194
00:18:35,600 --> 00:18:39,040
So the whole point of this is we're separating that away.

195
00:18:39,360 --> 00:18:43,840
So now the AI app or the agent can just focus on, hey, this is the knowledge base.

196
00:18:44,360 --> 00:18:47,680
and it's going to just give me all of the information I actually need.

197
00:18:50,200 --> 00:18:56,480
Now, what's really powerful here is how it uses these different knowledge sources.

198
00:18:57,040 --> 00:19:04,960
It's not just saying, okay, before we had a request and we searched and we got a set of results.

199
00:19:06,120 --> 00:19:14,040
This is not simply, well, now it searches whatever that search is across all of the different knowledge sources in the knowledge base.

200
00:19:14,840 --> 00:19:16,320
It's a lot richer than that.

201
00:19:17,000 --> 00:19:23,880
If we go and look at our knowledge base, we actually see a few different things straight away.

202
00:19:25,320 --> 00:19:30,760
So one of the things we have here is, well, we give it a chat completions model.

203
00:19:31,800 --> 00:19:34,200
So this is a generative model it can use

204
00:19:34,600 --> 00:19:40,080
as part of its reasoning, its planning, there's different options that I can leverage.

205
00:19:40,080 --> 00:19:48,440
And then it has this idea of a reasoning effort, minimal, low, or medium.

206
00:19:49,520 --> 00:19:54,920
Now you'll notice minimum is grayed out because I have it in a certain output mode.

207
00:19:54,960 --> 00:19:58,680
Actually, if I go and look at a different model for a second, that one's using web.

208
00:19:58,680 --> 00:20:01,800
Web forces you to use a certain mode.

209
00:20:02,520 --> 00:20:04,040
So here I can see all three of them.

210
00:20:05,000 --> 00:20:06,760
Minimal, low, and medium.

211
00:20:07,960 --> 00:20:14,040
So let's talk a little bit about exactly what is happening here, because it is a lot richer.

212
00:20:15,400 --> 00:20:21,880
So remember we talked about this idea of this agentic RAG, this multi-hop capability.

213
00:20:22,520 --> 00:20:28,360
So it's doing intelligent thinking to actually work out what it needs to do.

214
00:20:28,680 --> 00:20:32,120
So as part of this agentic RAG, it's

215
00:20:32,600 --> 00:20:36,920
not going to just use everything that exists in that knowledge base.

216
00:20:36,920 --> 00:20:38,360
Now, that's actually not true.

217
00:20:38,840 --> 00:20:43,960
If I set it to minimum, that minimum level of reasoning effort, then it does.

218
00:20:44,520 --> 00:20:50,120
It just takes the request you send it, and it sends exactly that request to all of the knowledge sources.

219
00:20:50,600 --> 00:20:54,360
So the effort you select does impact its behavior.

220
00:20:55,400 --> 00:20:58,200
But if I select an effort,

221
00:21:01,640 --> 00:21:09,840
of low or medium, well now it's going to do a few different things.

222
00:21:09,840 --> 00:21:11,640
It's going to work out a plan.

223
00:21:17,270 --> 00:21:24,630
And it's only going to use the knowledge sources what is needed.

224
00:21:29,590 --> 00:21:31,110
So that's with low and medium.

225
00:21:32,600 --> 00:21:36,280
So minimal, sure, whatever I ask it, it's just going to ask all of them.

226
00:21:37,480 --> 00:21:41,440
Low and medium, it's going to look at the knowledge source descriptions.

227
00:21:41,440 --> 00:21:52,760
It's going to look at the knowledge base retrieval instructions to assess, well based on what I'm being asked, what is the best way, the most efficient way,

228
00:21:53,560 --> 00:21:54,200
to work this out.

229
00:21:54,200 --> 00:22:04,320
And it's not just going to pass the query you asked it, it's going to go and maybe break that into sub-queries and send it to different knowledge sources to get the answer.

230
00:22:04,320 --> 00:22:05,800
That's where the agentic part comes in.

231
00:22:05,800 --> 00:22:10,040
So it's going to be selective about which knowledge sources it's going to use.

232
00:22:10,040 --> 00:22:13,720
If it's a really simple question, it may just pick a single knowledge source.

233
00:22:14,280 --> 00:22:18,280
If it's a more complex question, then sure, maybe it has to ask multiple knowledge sources.

234
00:22:18,840 --> 00:22:22,920
And it's why it's so important to have quality descriptions.

235
00:22:23,400 --> 00:22:28,760
So when I look at this, if I was, jump at my, I have multiple ones again.

236
00:22:30,840 --> 00:22:36,600
If I look at my knowledge source, so say YouTube, I give it a description.

237
00:22:38,360 --> 00:22:42,200
Transcripts from John Savill's technical training, YouTube channel content.

238
00:22:43,160 --> 00:22:51,160
Because that's the understanding that the model used to say which knowledge sources it should use

239
00:22:51,680 --> 00:22:53,640
That's how it's going to make that decision.

240
00:22:54,520 --> 00:22:56,600
This one is just search of web information.

241
00:22:56,600 --> 00:23:04,840
So I'm giving a description to the knowledge sources because it's going to help the model when it's trying to do its train of thought, when it's going to work out its plan.

242
00:23:05,160 --> 00:23:07,720
Well, which of the knowledge sources should I actually use?

243
00:23:07,720 --> 00:23:11,800
And then I can add in instructions to my knowledge base.

244
00:23:12,280 --> 00:23:13,160
So I'm guiding it.

245
00:23:14,040 --> 00:23:17,400
Look, if it's about technical items, prioritize using.

246
00:23:18,520 --> 00:23:21,480
my YouTube knowledge source first all the time.

247
00:23:22,040 --> 00:23:25,800
If you can't find an answer, then you're allowed to use KSWeb.

248
00:23:25,800 --> 00:23:36,600
So I'm giving it very specific guidance of how I want it to go about actually answering and solving that problem.

249
00:23:37,160 --> 00:23:42,720
So the whole point of this agentic, this multi-hop, is it's going to create a query plan, that chain of thought.

250
00:23:42,720 --> 00:23:49,320
It's going to work out different queries that are most applicable to the knowledge sources it's going to select, and then it executes it.

251
00:23:49,880 --> 00:24:00,440
Now what's nice here is if I do medium, it also then does a self-reflection.

252
00:24:03,440 --> 00:24:10,440
So what that means is it's done this plan, it's queried the particular knowledge sources it selected, gets the results back.

253
00:24:11,240 --> 00:24:17,800
With medium effort, it looks at those results, looks at what it was trying to do and says, have I solved it?

254
00:24:19,200 --> 00:24:26,120
If it doesn't think it has, it can do a second follow-up pass to try and get a better output.

255
00:24:26,120 --> 00:24:28,560
Now, obviously, it's going to use more tokens.

256
00:24:28,560 --> 00:24:30,040
It's going to have a longer lag.

257
00:24:30,920 --> 00:24:36,040
It's therefore going to cost more money, but I'm going to get a much higher quality of response.

258
00:24:36,360 --> 00:24:40,280
So you get to pick what is that effort I want to do.

259
00:24:42,120 --> 00:24:52,920
And Microsoft has a really nice sort of deep, techie article, and I'll put it in the link of the video, but it talks about that medium, low, and minimal.

260
00:24:54,200 --> 00:24:57,560
So if I do that minimal, it's

261
00:24:58,040 --> 00:24:59,680
Not doing any source selection.

262
00:24:59,680 --> 00:25:01,000
It's not doing any planning.

263
00:25:01,040 --> 00:25:01,800
It just doesn't do it.

264
00:25:02,200 --> 00:25:03,320
It just searches them all.

265
00:25:03,720 --> 00:25:04,760
Super, super basic.

266
00:25:06,600 --> 00:25:10,360
It doesn't use the web as a knowledge source grounding.

267
00:25:11,000 --> 00:25:16,320
But as soon as it goes to that low and medium, well, now it does do source selection.

268
00:25:16,320 --> 00:25:17,800
It does do query planning.

269
00:25:18,600 --> 00:25:24,920
And then really the big difference is we get this reflective additional retrieval step.

270
00:25:27,080 --> 00:25:32,200
and additional classifying as part of those responses.

271
00:25:32,760 --> 00:25:36,520
So we get additional rich capabilities on top of that.

272
00:25:39,160 --> 00:25:53,320
Now, another thing we get is, you may have noticed as part of the knowledge base configuration, this time it's grayed out.

273
00:25:53,640 --> 00:25:55,400
It's just answer synthesis.

274
00:25:56,520 --> 00:26:02,520
The reason it's grayed out and it's answer synthesis is because one of my knowledge sources is web.

275
00:26:03,480 --> 00:26:11,000
If it's web, it has to be answer synthesis and it won't let me do minimal reasoning.

276
00:26:12,840 --> 00:26:20,040
But if I don't have web, I have a choice.

277
00:26:21,480 --> 00:26:25,960
I have extractive data and I have answer.

278
00:26:26,200 --> 00:26:26,920
Synthesis.

279
00:26:28,960 --> 00:26:33,560
Answer synthesis is only available with low and medium.

280
00:26:34,520 --> 00:26:38,920
So if I change this to answer synthesis, you'll notice I can't select minimal anymore.

281
00:26:40,040 --> 00:26:41,960
Go back to extractive data for now.

282
00:26:42,280 --> 00:26:43,560
So what is this all about?

283
00:26:43,640 --> 00:26:46,120
What is this pointive?

284
00:26:49,000 --> 00:26:53,720
We talked about before we have a particular API request.

285
00:26:54,320 --> 00:26:57,560
And that particular API request was just asking indexes.

286
00:26:57,880 --> 00:26:59,640
And that's why it would ask.

287
00:27:00,760 --> 00:27:08,440
With Boundary IQ, my API, it's still the Azure AI search resource.

288
00:27:08,440 --> 00:27:18,360
But instead of it being indexes, what we're going to change it to most of the time is knowledge bases.

289
00:27:19,000 --> 00:27:20,080
There's also knowledge sources.

290
00:27:20,080 --> 00:27:21,560
I think there's knowledge retrievals.

291
00:27:23,560 --> 00:27:29,080
But as part of that, I have two choices into how it returns.

292
00:27:30,120 --> 00:27:41,640
So option one is extractive data.

293
00:27:43,080 --> 00:27:48,600
And that's kind of what we're used to in the old Azure AI search.

294
00:27:50,120 --> 00:27:51,760
I'm going to make some requests.

295
00:27:51,760 --> 00:28:03,160
A very common this would be, for example, if this was Foundry, and it was the agent service.

296
00:28:04,680 --> 00:28:06,360
So I have a generative model.

297
00:28:07,000 --> 00:28:10,840
It just is asking for, give me some data.

298
00:28:11,560 --> 00:28:13,240
I'm then going to reason over it.

299
00:28:13,240 --> 00:28:16,280
I'm then going to come up with the answers because I'm using it a certain way.

300
00:28:16,600 --> 00:28:20,280
I just want you to give me a set of data back that's most applicable.

301
00:28:21,240 --> 00:28:24,360
So this option one is just extractive data.

302
00:28:24,800 --> 00:28:26,520
And we'll let's see this in action.

303
00:28:27,320 --> 00:28:28,600
So if we jump back over.

304
00:28:28,920 --> 00:28:30,680
So right now I have it in that mode.

305
00:28:31,960 --> 00:28:33,000
Extractive data.

306
00:28:34,120 --> 00:28:39,880
So if I go and look at my agent and I'm using that

307
00:28:40,040 --> 00:28:41,120
knowledge base right now.

308
00:28:41,120 --> 00:28:42,520
You can see I've added knowledge.

309
00:28:43,000 --> 00:28:44,360
It's that just basic one.

310
00:28:45,200 --> 00:28:48,680
If I go and ask it a question, what is ExpressRoute?

311
00:28:53,640 --> 00:28:55,720
So it's going to go and use that knowledge base.

312
00:28:58,200 --> 00:28:59,880
It's asking my approval.

313
00:29:00,040 --> 00:29:05,240
Now you can change it so you don't have to keep doing this, but I wanted you to see it.

314
00:29:05,240 --> 00:29:07,800
So I'm going to keep approving only once.

315
00:29:09,520 --> 00:29:14,280
So that's going to go and call that knowledge base with these requests.

316
00:29:17,400 --> 00:29:18,520
And we'll get the answer back.

317
00:29:20,360 --> 00:29:28,280
But what I want to do is go and look at the debug to see exactly what gets sent back from that Azure AI search request.

318
00:29:30,200 --> 00:29:38,200
So if I look at debug and I actually look at the usage of the knowledge base, so we can see it made the request.

319
00:29:39,200 --> 00:29:40,520
But look at what we get back.

320
00:29:40,560 --> 00:29:44,360
What we get back is just a set of chunks.

321
00:29:45,880 --> 00:29:48,880
We got 11 documents retrieved.

322
00:29:48,880 --> 00:29:53,720
It's just sending us a bunch of data back.

323
00:29:54,280 --> 00:29:55,160
That's all we get.

324
00:29:56,280 --> 00:30:03,640
And then my model can do what it normally does is use that as part of its decision.

325
00:30:04,440 --> 00:30:04,760
Great.

326
00:30:06,120 --> 00:30:07,640
The other option I can do

327
00:30:09,240 --> 00:30:11,800
is I can think about answer synthesis.

328
00:30:12,360 --> 00:30:18,840
So with answer synthesis, imagine now it is something else.

329
00:30:19,240 --> 00:30:20,680
It's just maybe a chat app.

330
00:30:20,680 --> 00:30:21,840
So I've written my own little chat.

331
00:30:21,840 --> 00:30:23,840
Actually, I don't want to use that color, different color.

332
00:30:26,200 --> 00:30:28,640
So I just write some kind of chat application.

333
00:30:28,640 --> 00:30:33,080
And my chat app wants to make a response.

334
00:30:33,080 --> 00:30:34,600
And really, I just want the answer back.

335
00:30:35,480 --> 00:30:37,720
So here, my second option

336
00:30:38,840 --> 00:30:44,280
is that answer synthesis.

337
00:30:45,400 --> 00:30:52,280
It's going to give me a generated answer based on the various knowledge things it finds.

338
00:30:52,520 --> 00:30:54,600
So then it can just return it as is.

339
00:30:54,840 --> 00:30:56,920
It doesn't have to then go and do anything else.

340
00:30:56,920 --> 00:30:58,600
It just wants me to give it the answer.

341
00:30:59,160 --> 00:31:02,920
So if now if we go and look, and this time I'll change it.

342
00:31:03,800 --> 00:31:06,120
So remember, we are other knowledge sources.

343
00:31:06,120 --> 00:31:07,720
Either I could just change my existing one.

344
00:31:08,680 --> 00:31:16,360
But if I look at my second one that uses both my index and the web, this one, because it's using the web, is always doing answer synthesis.

345
00:31:17,160 --> 00:31:25,880
So I would change my agent to not use that, and instead we'll add that one.

346
00:31:29,320 --> 00:31:30,680
We'll ask it the same question.

347
00:31:37,120 --> 00:31:40,360
And what we should see this time, yep, prove.

348
00:31:42,200 --> 00:31:49,160
is we'll actually get a generated answer and then it will tell us the information it used as references.

349
00:31:49,640 --> 00:31:55,480
But it's actually going to give the answer as part of its response in that natural language.

350
00:31:55,480 --> 00:31:56,920
It's going to work it out for us.

351
00:31:56,920 --> 00:31:59,080
So it's doing that full answer synthesis.

352
00:31:59,480 --> 00:32:04,480
So I as a more basic AI app wouldn't have to then do anything else.

353
00:32:04,480 --> 00:32:06,000
It's just giving me the complete answer.

354
00:32:06,000 --> 00:32:08,360
And you can see it's taking a little bit longer.

355
00:32:10,680 --> 00:32:12,360
And it's throwing out some output.

356
00:32:13,080 --> 00:32:27,800
But now if we look at the debug and I look at what it actually did this time, if I look at the information, it's giving me the description.

357
00:32:28,120 --> 00:32:29,080
This is the answer.

358
00:32:29,640 --> 00:32:31,720
It's just throwing out a complete answer.

359
00:32:31,960 --> 00:32:37,080
And then it's telling me the various sources it used.

360
00:32:38,720 --> 00:32:41,240
So it behaves in a very different way.

361
00:32:42,440 --> 00:32:49,720
So you pick, you decide what is it you want it to do based on how is it going to be used.

362
00:32:50,040 --> 00:32:55,080
So the key point here, again, if I'm writing a rich agent, I probably just want the data back.

363
00:32:55,240 --> 00:32:58,920
I don't want it to then extrapolate out answers.

364
00:32:59,240 --> 00:33:02,840
So I would want to just use the extractive data option for it.

365
00:33:02,840 --> 00:33:05,520
Just give me the information that's most relevant.

366
00:33:06,280 --> 00:33:07,960
Hey, a more basic chat thing.

367
00:33:07,960 --> 00:33:11,160
Sure, maybe I actually want it to go ahead and create the full answer for me.

368
00:33:11,720 --> 00:33:16,320
There's actually a nicer demonstration someone in the product group wrote, so we'll look at this.

369
00:33:16,320 --> 00:33:18,320
So this is doing the same thing.

370
00:33:18,320 --> 00:33:19,480
It's using a knowledge base.

371
00:33:19,480 --> 00:33:26,760
If I look at the settings, you can see they've got three different knowledge sources, BLOB, OneLake, and the web.

372
00:33:28,760 --> 00:33:31,800
Because it's using the web, it's going to have to be answer synthesis.

373
00:33:33,040 --> 00:33:37,080
It's using a medium reasoning effort and you can just ask it a certain question.

374
00:33:38,680 --> 00:33:49,640
But what this actually does is they've added some additional information about what Azure AI Search I Foundry IQ is actually doing behind the scenes to get the answer.

375
00:33:49,680 --> 00:33:54,360
And it can show you the number of passes it's doing, the iterations.

376
00:33:54,360 --> 00:33:58,440
It's a really cool just way of seeing everything that is going on.

377
00:33:58,920 --> 00:34:00,440
So here I can see, okay,

378
00:34:02,760 --> 00:34:10,840
It did 2 iterations, so two distinct planning cycles, 11 different activities, 60 different retrieval calls.

379
00:34:10,840 --> 00:34:13,400
I can see the lapsed time, the planning tokens.

380
00:34:15,240 --> 00:34:29,720
I can see the query planning steps, what it actually did as part of the requests, what it did on the second iteration, some additional queries it made, and then spat out the answer and all of the various references to it.

381
00:34:30,600 --> 00:34:36,440
So it's just an example that you can kind of see some of the power that is really going into it.

382
00:34:38,200 --> 00:34:48,200
So in summary, the whole point here is that Foundry IQ brings the ability to just query the knowledge base.

383
00:34:48,200 --> 00:34:58,520
And that knowledge base gives me that single queryable entity that will then intelligently utilize all the available knowledge sources that can be, hey, local,

384
00:35:00,280 --> 00:35:14,360
I can use remote, and I've which just gives me a list of the most relevant data, so I get a high quality inferencing from my agent or app, or it can just synthesize the full answer.

385
00:35:15,440 --> 00:35:18,680
And that's the whole goal of all of this.

386
00:35:20,520 --> 00:35:25,480
You see these interesting things about data and information and knowledge and wisdom.

387
00:35:26,000 --> 00:35:29,000
And the goal of all of this is to build on top of that.

388
00:35:29,000 --> 00:35:31,160
So we're used to the idea of just like raw data.

389
00:35:33,400 --> 00:35:40,680
And then we do certain amount of processing to actually get useful information that we can then use.

390
00:35:40,680 --> 00:35:44,760
It's why we bring data into OneLake, for example, in Fabric.

391
00:35:45,880 --> 00:35:49,120
But then we add additional context.

392
00:35:49,120 --> 00:35:54,200
We consolidate, we group it, we interpret it.

393
00:35:54,960 --> 00:35:57,320
which then gives us the idea of knowledge.

394
00:35:58,520 --> 00:36:04,120
And it's only once we have knowledge, we can actually reason over it.

395
00:36:04,840 --> 00:36:08,680
It's only once we have knowledge that I can make judgments based on it.

396
00:36:08,680 --> 00:36:15,320
So that's that idea of wisdom, which we can apply once we have that knowledge.

397
00:36:15,640 --> 00:36:17,560
And that's what Foundry IQ is doing.

398
00:36:17,560 --> 00:36:19,120
Foundry IQ is giving us knowledge.

399
00:36:19,120 --> 00:36:20,840
That's why they call it knowledge bases.

400
00:36:20,840 --> 00:36:24,040
It's knowledge we can now leverage in the same way

401
00:36:25,080 --> 00:36:41,240
that I can think about, well, Fabric IQ does the same thing for the data in OneLake or shortcut to OneLake via its ontology to make it actually map to real enterprise entities.

402
00:36:41,640 --> 00:36:43,640
It's the same thing Work IQ.

403
00:36:44,920 --> 00:36:52,840
is doing for all of the data, yes, in M365, but it adds memory, customization, richer inferencing.

404
00:36:53,080 --> 00:36:55,280
It's all about giving you knowledge.

405
00:36:55,280 --> 00:37:07,240
And then once I have these different now knowledge sources, then the AI apps, the AI agents I write,

406
00:37:11,880 --> 00:37:14,760
then can provide the wisdom on it.

407
00:37:15,560 --> 00:37:16,760
And that's the goal.

408
00:37:16,760 --> 00:37:23,960
So your AI app and agent now gets quality knowledge across these so it can now do the right things.

409
00:37:23,960 --> 00:37:25,640
And the whole point is you use them together.

410
00:37:25,960 --> 00:37:29,560
The work IQ would give you an understanding of the full user context.

411
00:37:30,120 --> 00:37:32,840
Fabric IQ would give you full system operational knowledge.

412
00:37:33,080 --> 00:37:36,560
Founder IQ would give you the full knowledge around any organizational information.

413
00:37:36,560 --> 00:37:37,480
So together,

414
00:37:37,960 --> 00:37:42,520
it can really answer any question for your AI apps and agents.

415
00:37:43,320 --> 00:37:43,880
And that was it.

416
00:37:44,640 --> 00:37:45,640
I hope that was useful.

417
00:37:46,040 --> 00:37:47,560
As always, till the next video, take care.