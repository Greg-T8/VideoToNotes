[
  {
    "section_title": "\ud83c\udfa4 [00:00:15] AI models and their knowledge",
    "timestamp_range": "00:00:21 \u2013 00:01:57",
    "level": 3,
    "order": 0,
    "content": "### \ud83c\udfa4 [00:00:15] AI models and their knowledge  \n**Timestamp**: 00:00:21 \u2013 00:01:57  \n\n**Key Concepts**  \n- Generative AI models are trained on a large but finite corpus of data (books, websites, transcripts).  \n- Training data has a cutoff date and excludes non-public information.  \n- Model training adjusts neuron weights and biases to predict next tokens for generating responses.  \n- Models cannot access information outside their training data unless explicitly provided during a request.  \n- To use external or updated data, the AI application must retrieve relevant information and include it in the prompt.  \n- This process of augmenting model input with external data is called Retrieval-Augmented Generation (RAG).  \n- The quality and relevance of the retrieved information directly impact the quality of the model\u2019s output.  \n\n**Definitions**  \n- **Generative Model**: A model trained to predict the next most probable token in a sequence to generate text.  \n- **Retrieval-Augmented Generation (RAG)**: Technique of retrieving external information to augment the model\u2019s knowledge during inference.  \n\n**Key Facts**  \n- Training data is finite and has a cutoff date.  \n- External data must be provided at request time to supplement the model\u2019s knowledge.  \n\n**Examples**  \n- An internal company database can be queried first to retrieve relevant data, which is then added to the prompt for the generative model.  \n\n**Exam Tips \ud83c\udfaf**  \n- Understand the limitations of pre-trained generative models regarding knowledge cutoff and private data.  \n- Know the concept and importance of RAG for augmenting model responses with external information.  \n- Remember: \u201cGarbage in, garbage out\u201d applies to the quality of retrieved data affecting output quality.  \n\n---\n\n"
  },
  {
    "section_title": "\ud83c\udfa4 [00:01:31] RAG to the rescue",
    "timestamp_range": "00:01:58 \u2013 00:03:19",
    "level": 3,
    "order": 0,
    "content": "### \ud83c\udfa4 [00:01:31] RAG to the rescue  \n**Timestamp**: 00:01:58 \u2013 00:03:19  \n\n**Key Concepts**  \n- RAG involves the AI app first querying an external knowledge source to retrieve relevant information.  \n- The retrieved information is added to the prompt sent to the generative model.  \n- Azure AI Search is a key tool to implement RAG by exposing an API endpoint for querying indexes of data.  \n- Azure AI Search creates indexes for different data sources and returns the best matching data for a query.  \n- This enables the model to generate responses using up-to-date and relevant information beyond its training data.  \n\n**Definitions**  \n- **Azure AI Search**: A cloud service that indexes data sources and provides search capabilities via API for RAG.  \n- **Index**: A structured representation of a data source optimized for search queries.  \n\n**Key Facts**  \n- Azure AI Search supports querying multiple types of data sources by creating indexes.  \n- The app sends a query to Azure AI Search, which returns relevant data to augment the prompt.  \n\n**Examples**  \n- An AI app sends a user question to Azure AI Search API, which searches an index and returns relevant data to include in the prompt.  \n\n**Exam Tips \ud83c\udfaf**  \n- Be able to explain the role of Azure AI Search in enabling RAG.  \n- Understand the flow: user query \u2192 search index \u2192 retrieved info \u2192 augmented prompt \u2192 model response.  \n\n---\n\n"
  },
  {
    "section_title": "\ud83c\udfa4 [00:03:12] Azure AI Search",
    "timestamp_range": "00:03:20 \u2013 00:08:22",
    "level": 3,
    "order": 0,
    "content": "### \ud83c\udfa4 [00:03:12] Azure AI Search  \n**Timestamp**: 00:03:20 \u2013 00:08:22  \n\n**Key Concepts**  \n- Azure AI Search creates indexes for specific data sources (blobs, databases, etc.).  \n- It supports both lexical (keyword) search and semantic search using vector embeddings.  \n- Lexical search matches exact keywords; semantic search matches meaning using high-dimensional vectors.  \n- Data is chunked and embedded into vectors representing semantic meaning.  \n- Queries are also embedded and matched against data vectors to find semantically closest results.  \n- Azure AI Search combines lexical and semantic rankings using reciprocal rank fusion and re-ranking for confidence scoring.  \n- Each search is performed against a single index representing one information source.  \n- This is considered the RAG 1.0 approach (single-hop retrieval).  \n\n**Definitions**  \n- **Lexical Search**: Keyword-based search matching exact terms.  \n- **Semantic Search**: Search based on meaning using vector embeddings.  \n- **Embedding Model**: A model that converts text chunks into high-dimensional vectors representing semantic meaning.  \n- **Reciprocal Rank Fusion**: A method to combine rankings from different search methods into a single ranked list.  \n\n**Key Facts**  \n- Azure AI Search indexes one data source per index.  \n- Semantic search helps handle natural language nuances like idioms and synonyms.  \n- The search process includes creating embeddings for both data and queries.  \n\n**Examples**  \n- Searching for \u201craining cats and dogs\u201d uses semantic search to understand it means heavy rain, not literal animals.  \n\n**Exam Tips \ud83c\udfaf**  \n- Know the difference between lexical and semantic search.  \n- Understand how vector embeddings enable semantic search.  \n- Be able to describe how Azure AI Search combines lexical and semantic results.  \n- Remember that RAG 1.0 uses single index searches.  \n\n---\n\n"
  },
  {
    "section_title": "\ud83c\udfa4 [00:08:24] Foundry IQ",
    "timestamp_range": "00:08:22 \u2013 00:09:59",
    "level": 3,
    "order": 0,
    "content": "### \ud83c\udfa4 [00:08:24] Foundry IQ  \n**Timestamp**: 00:08:22 \u2013 00:09:59  \n\n**Key Concepts**  \n- Foundry IQ builds on Azure AI Search to provide a true knowledge layer, not just information retrieval.  \n- Moves from single-shot RAG (one index) to agentic RAG with multi-hop capabilities.  \n- Supports searching across multiple knowledge sources in a single request.  \n- Knowledge sources can be grouped into knowledge bases for combined querying.  \n- Enables richer, more intelligent retrieval by considering multiple data sources simultaneously.  \n\n**Definitions**  \n- **Agentic RAG**: Retrieval augmented generation with intelligent planning and multi-hop querying across multiple knowledge sources.  \n- **Knowledge Source**: An individual indexed data source (e.g., Azure AI Search index, SharePoint site).  \n- **Knowledge Base**: A collection of knowledge sources grouped for querying.  \n\n**Key Facts**  \n- Foundry IQ allows multi-source search in one request, unlike single index searches in RAG 1.0.  \n- Knowledge bases aggregate multiple knowledge sources for richer context.  \n\n**Examples**  \n- Querying both a product database index and a policy document index together to answer complex questions.  \n\n**Exam Tips \ud83c\udfaf**  \n- Understand the difference between single-shot RAG and agentic RAG.  \n- Know the concepts of knowledge sources and knowledge bases in Foundry IQ.  \n- Be able to explain multi-hop querying across multiple knowledge sources.  \n\n---\n\n"
  },
  {
    "section_title": "\ud83c\udfa4 [00:09:32] Multiple knowledge sources",
    "timestamp_range": "00:09:59 \u2013 00:11:55",
    "level": 3,
    "order": 0,
    "content": "### \ud83c\udfa4 [00:09:32] Multiple knowledge sources  \n**Timestamp**: 00:09:59 \u2013 00:11:55  \n\n**Key Concepts**  \n- Knowledge sources can include various data types: Azure AI Search indexes, Fabric\u2019s OneLake data with Fabric IQ ontology, SharePoint sites, etc.  \n- Fabric IQ adds semantic meaning by modeling enterprise entities, relationships, and properties.  \n- SharePoint data can be indexed locally or accessed via remote semantic indexes.  \n- Remote knowledge sources allow querying external semantic indexes without local indexing.  \n- The AI app or user is unaware of whether data is local or remote; it\u2019s transparent.  \n\n**Definitions**  \n- **Fabric IQ**: Fabric\u2019s ontology layer providing semantic models of enterprise data.  \n- **Remote Knowledge Source**: A knowledge source accessed via external semantic indexes or APIs rather than local indexing.  \n\n**Key Facts**  \n- Local indexing creates Azure AI Search indexes with embeddings.  \n- Remote sources include M365 Work IQ semantic index, web search via Bing, and MCP protocol endpoints.  \n- Permissions and metadata are preserved when indexing SharePoint data.  \n\n**Examples**  \n- Using Fabric IQ to semantically index enterprise data in OneLake.  \n- Accessing SharePoint data via M365\u2019s semantic index without local indexing.  \n- Querying the web via Bing as a remote knowledge source.  \n\n**Exam Tips \ud83c\udfaf**  \n- Know the difference between local and remote knowledge sources.  \n- Understand how Fabric IQ enriches data with semantic enterprise models.  \n- Be aware of the transparency of knowledge source location to the AI app.  \n\n---\n\n"
  },
  {
    "section_title": "\ud83c\udfa4 [00:14:22] Knowledge bases and use of Azure AI Search resource",
    "timestamp_range": "00:14:20 \u2013 00:15:44",
    "level": 3,
    "order": 0,
    "content": "### \ud83c\udfa4 [00:14:22] Knowledge bases and use of Azure AI Search resource  \n**Timestamp**: 00:14:20 \u2013 00:15:44  \n\n**Key Concepts**  \n- A Foundry IQ knowledge base is created within an existing Azure AI Search resource.  \n- Knowledge bases consist of multiple knowledge sources (up to 10 currently).  \n- Knowledge bases are the main interface for AI apps to query grouped knowledge sources.  \n- Knowledge sources can be reused across multiple knowledge bases.  \n- Knowledge bases represent a curated collection of knowledge relevant to a domain or enterprise entity.  \n\n**Definitions**  \n- **Azure AI Search Resource**: The cloud service instance where indexes and knowledge bases are created.  \n- **Knowledge Base**: A collection of knowledge sources grouped for querying by AI apps.  \n\n**Key Facts**  \n- Knowledge bases are created on top of Azure AI Search resources.  \n- Current limit is 10 knowledge sources per knowledge base (subject to change).  \n- Knowledge sources can be existing indexes or newly created from blobs, SharePoint, web, etc.  \n\n**Examples**  \n- A knowledge base containing an Azure AI Search index and a web knowledge source.  \n\n**Exam Tips \ud83c\udfaf**  \n- Understand the hierarchy: Azure AI Search resource \u2192 knowledge bases \u2192 knowledge sources.  \n- Know that knowledge sources can be shared across knowledge bases.  \n- Remember knowledge bases are what AI apps interact with, not individual knowledge sources.  \n\n---\n\n"
  },
  {
    "section_title": "\ud83c\udfa4 [00:15:44] Adding knowledge sources",
    "timestamp_range": "00:15:44 \u2013 00:17:09",
    "level": 3,
    "order": 0,
    "content": "### \ud83c\udfa4 [00:15:44] Adding knowledge sources  \n**Timestamp**: 00:15:44 \u2013 00:17:09  \n\n**Key Concepts**  \n- Knowledge sources can be added to knowledge bases by creating new ones or selecting existing ones.  \n- Types of knowledge sources include: Azure AI Search indexes, blob containers, web, specific SharePoint sites, general SharePoint (using M365 semantic index), Fabric OneLake, and MCP (private preview).  \n- Adding a knowledge source involves indexing data (chunking, embedding) or connecting to remote semantic indexes.  \n- MCP protocol allows AI apps to discover and use external knowledge/tools via a standard interface.  \n\n**Definitions**  \n- **MCP (Microsoft Copilot Protocol)**: A standard protocol to expose knowledge and tools to AI applications.  \n\n**Key Facts**  \n- MCP support is in private preview and requires signup.  \n- SharePoint general knowledge source uses M365 Work IQ semantic index (remote).  \n- Blob containers and OneLake data are indexed locally in Azure AI Search.  \n\n**Examples**  \n- Adding a blob container as a knowledge source creates an index with embeddings.  \n- Pointing to a SharePoint site creates a local index; pointing to SharePoint generally uses remote semantic index.  \n\n**Exam Tips \ud83c\udfaf**  \n- Know the different types of knowledge sources and how they are added.  \n- Understand the difference between local indexing and remote semantic indexing.  \n- Be aware of MCP as an emerging standard for knowledge/tool integration.  \n\n---\n\n"
  },
  {
    "section_title": "\ud83c\udfa4 [00:17:09] SKU limits",
    "timestamp_range": "00:17:09 \u2013 00:17:46",
    "level": 3,
    "order": 0,
    "content": "### \ud83c\udfa4 [00:17:09] SKU limits  \n**Timestamp**: 00:17:09 \u2013 00:17:46  \n\n**Key Concepts**  \n- Azure AI Search SKUs impose limits on the number of knowledge bases and knowledge sources.  \n- Free SKU supports 3 knowledge bases and 3 knowledge sources.  \n- Basic SKU supports 15 knowledge bases and sources (may vary if created earlier).  \n- Higher SKUs (S1, S2, S3, etc.) support progressively larger numbers of knowledge bases and sources.  \n\n**Definitions**  \n- **SKU (Stock Keeping Unit)**: Different tiers of Azure AI Search service with varying capabilities and limits.  \n\n**Key Facts**  \n- SKU limits affect how many knowledge bases and sources can be created and used.  \n- Users should check current SKU limits as they may change.  \n\n**Examples**  \n- Free tier: 3 knowledge bases and 3 knowledge sources max.  \n- Basic tier: 15 knowledge bases and sources max.  \n\n**Exam Tips \ud83c\udfaf**  \n- Know that SKU tier determines capacity for knowledge bases and sources.  \n- Remember to verify SKU limits when planning deployments.  \n\n---\n\n"
  },
  {
    "section_title": "\ud83c\udfa4 [00:17:46] Collections of knowledge sources",
    "timestamp_range": "00:17:46 \u2013 00:18:49",
    "level": 3,
    "order": 0,
    "content": "### \ud83c\udfa4 [00:17:46] Collections of knowledge sources  \n**Timestamp**: 00:17:46 \u2013 00:18:49  \n\n**Key Concepts**  \n- A knowledge base is essentially a collection of knowledge sources grouped together.  \n- This grouping represents useful knowledge about enterprise entities or topics.  \n- AI apps or agents query knowledge bases rather than individual knowledge sources.  \n- This abstraction simplifies agent design by offloading multi-source querying complexity.  \n\n**Definitions**  \n- **Knowledge Base**: A curated set of knowledge sources representing a domain or topic.  \n\n**Key Facts**  \n- Knowledge bases enable seamless querying across multiple data sources.  \n- Agents no longer need to manage multiple queries and data aggregation themselves.  \n\n**Examples**  \n- A knowledge base combining blobs, SharePoint, and web sources for policy and product information.  \n\n**Exam Tips \ud83c\udfaf**  \n- Understand the purpose of knowledge bases as collections of knowledge sources.  \n- Recognize how knowledge bases simplify AI app and agent interactions with data.  \n\n---\n\n"
  },
  {
    "section_title": "\ud83c\udfa4 [00:18:49] Reasoning effort",
    "timestamp_range": "00:18:49 \u2013 00:24:03",
    "level": 3,
    "order": 0,
    "content": "### \ud83c\udfa4 [00:18:49] Reasoning effort  \n**Timestamp**: 00:18:49 \u2013 00:24:03  \n\n**Key Concepts**  \n- Foundry IQ supports different reasoning effort levels: minimal, low, medium.  \n- Minimal effort: sends the user query as-is to all knowledge sources without filtering.  \n- Low/Medium effort: the system plans and selects which knowledge sources to query based on descriptions and instructions.  \n- The system may break queries into sub-queries and selectively query knowledge sources (agentic RAG).  \n- Knowledge source descriptions and knowledge base retrieval instructions guide source selection and query planning.  \n- Medium effort adds self-reflection: after querying, the system evaluates if the question is answered and can re-plan if needed.  \n\n**Definitions**  \n- **Reasoning Effort**: The level of intelligent planning and selectivity applied when querying knowledge sources.  \n- **Self-Reflection**: The process of evaluating if the retrieved information sufficiently answers the query and adjusting accordingly.  \n\n**Key Facts**  \n- Minimal effort queries all sources indiscriminately.  \n- Low and medium effort use metadata and instructions to optimize querying.  \n- Self-reflection is only available at medium effort level.  \n\n**Examples**  \n- Descriptions like \u201ctranscripts from John S\u2019s technical training YouTube channel\u201d help the system decide when to use that knowledge source.  \n- Instructions can prioritize certain sources for specific topics (e.g., technical questions first use YouTube source).  \n\n**Exam Tips \ud83c\udfaf**  \n- Know the differences between minimal, low, and medium reasoning effort.  \n- Understand how descriptions and instructions influence source selection.  \n- Be able to explain the concept of agentic RAG and self-reflection in Foundry IQ."
  },
  {
    "section_title": "\ud83c\udfa4 [00:25:39] Output modes",
    "timestamp_range": "00:24:40 \u2013 00:34:38",
    "level": 3,
    "order": 0,
    "content": "### \ud83c\udfa4 [00:25:39] Output modes  \n**Timestamp**: 00:24:40 \u2013 00:34:38\n\n**Key Concepts**  \n- Different reasoning effort levels affect output quality, latency, and cost: minimal, low, medium.  \n- Minimal reasoning: no source selection or planning; searches all sources indiscriminately; no web grounding.  \n- Low and medium reasoning: include source selection, query planning, and reflective retrieval steps for richer responses.  \n- Answer modes:  \n  - Extractive data: returns raw relevant data chunks from knowledge bases; suitable for advanced agents that do their own reasoning.  \n  - Answer synthesis: returns a fully generated natural language answer synthesized from multiple knowledge sources; suitable for simpler chat apps.  \n- Web as a knowledge source requires answer synthesis mode and disallows minimal reasoning.  \n- Knowledge base API requests can target indexes or knowledge bases; knowledge bases unify multiple knowledge sources.  \n- Debugging output shows the difference between extractive data (multiple documents/chunks returned) and answer synthesis (single synthesized answer with source references).  \n- Choice of output mode depends on application needs: detailed data for complex agents vs. direct answers for basic chat apps.\n\n**Definitions**  \n- **Minimal reasoning**: Basic search without source selection or query planning.  \n- **Low/Medium reasoning**: Enhanced search with source selection, query planning, and reflective retrieval.  \n- **Extractive data**: Returning raw relevant documents or data chunks from knowledge bases without synthesis.  \n- **Answer synthesis**: Generating a natural language answer synthesized from multiple knowledge sources.\n\n**Key Facts**  \n- Minimal reasoning mode does not support web as a knowledge source.  \n- Answer synthesis mode is only available with low and medium reasoning efforts.  \n- Extractive data mode returns multiple documents (e.g., 11 chunks in example) for downstream reasoning.  \n- Answer synthesis mode returns a single generated answer with references and takes longer to process.\n\n**Examples**  \n- Asking \"What is express route?\" using extractive data mode returns multiple relevant documents for the agent to reason over.  \n- Asking the same question with answer synthesis mode returns a fully formed natural language answer along with source citations.  \n- Demo with three knowledge sources (blob, one lake, web) uses answer synthesis and medium reasoning effort, showing multiple iterations, retrieval calls, and planning steps in debug output.\n\n**Exam Tips \ud83c\udfaf**  \n- Understand the trade-offs between minimal, low, and medium reasoning efforts in terms of cost, latency, and output quality.  \n- Know when to use extractive data (for complex agents) vs. answer synthesis (for simpler chat apps).  \n- Remember web knowledge sources require answer synthesis and disallow minimal reasoning.  \n- Be able to interpret debug outputs showing retrieval chunks vs. synthesized answers.\n\n---\n\n"
  },
  {
    "section_title": "\ud83c\udfa4 [00:33:11] Peeking inside its thinking",
    "timestamp_range": "00:33:11 \u2013 00:34:38",
    "level": 3,
    "order": 0,
    "content": "### \ud83c\udfa4 [00:33:11] Peeking inside its thinking  \n**Timestamp**: 00:33:11 \u2013 00:34:38\n\n**Key Concepts**  \n- Foundry IQ provides detailed transparency into the internal reasoning and retrieval process behind answers.  \n- Shows number of iterations (planning cycles), activities, retrieval calls, and token usage.  \n- Displays query planning steps and additional queries made during multiple iterations.  \n- Outputs the final synthesized answer along with all referenced sources.  \n- Enables understanding of the complexity and depth of the AI\u2019s reasoning process.\n\n**Definitions**  \n- **Iteration**: A distinct planning and retrieval cycle during query processing.  \n- **Activity**: Individual steps or operations performed during reasoning.  \n- **Retrieval call**: A request made to knowledge sources to fetch relevant data.\n\n**Key Facts**  \n- Example demo showed 2 iterations, 11 activities, and 60 retrieval calls.  \n- Debug output includes elapsed time, planning tokens, and detailed query plans.\n\n**Examples**  \n- The demo application using Foundry IQ showed the full reasoning trace for a question, revealing multiple passes and retrievals before final answer synthesis.\n\n**Exam Tips \ud83c\udfaf**  \n- Be familiar with how AI systems can provide transparency into their reasoning steps.  \n- Understand the significance of multiple iterations and retrieval calls in improving answer quality.  \n- Know that detailed debug info can help diagnose and optimize AI query performance.\n\n---\n\n"
  },
  {
    "section_title": "\ud83c\udfa4 [00:34:37] Summary",
    "timestamp_range": "00:34:37 \u2013 00:35:15",
    "level": 3,
    "order": 0,
    "content": "### \ud83c\udfa4 [00:34:37] Summary  \n**Timestamp**: 00:34:37 \u2013 00:35:15\n\n**Key Concepts**  \n- Foundry IQ enables querying a unified knowledge base that intelligently integrates multiple knowledge sources (local and remote).  \n- Provides either a list of relevant data chunks or a fully synthesized answer.  \n- The goal is to deliver high-quality inferencing and knowledge to AI agents and applications.\n\n**Definitions**  \n- **Knowledge base**: A single queryable entity aggregating multiple knowledge sources for AI consumption.\n\n**Key Facts**  \n- Foundry IQ supports both extractive and synthesis modes for flexible AI application needs.\n\n**Examples**  \n- None in this chunk.\n\n**Exam Tips \ud83c\udfaf**  \n- Remember Foundry IQ\u2019s role as a knowledge integrator providing high-quality data or answers.  \n- Understand the distinction between data retrieval and answer synthesis in AI knowledge querying.\n\n---\n\n"
  },
  {
    "section_title": "\ud83c\udfa4 [00:35:15] How the IQs work together",
    "timestamp_range": "00:35:15 \u2013 00:37:40",
    "level": 3,
    "order": 0,
    "content": "### \ud83c\udfa4 [00:35:15] How the IQs work together  \n**Timestamp**: 00:35:15 \u2013 00:37:40\n\n**Key Concepts**  \n- Different IQs serve complementary roles:  \n  - Foundry IQ: organizational knowledge and information.  \n  - Fabric IQ: system operational knowledge and data in OneLake with ontology mapping.  \n  - Work IQ: user context, memory customization, and richer inferencing in M365.  \n- Together, these IQs provide comprehensive knowledge coverage for AI apps and agents.  \n- AI apps and agents leverage these IQs to gain knowledge and then apply reasoning to provide wisdom.  \n- The knowledge hierarchy: data \u2192 information \u2192 knowledge \u2192 wisdom.  \n- Foundry IQ focuses on delivering knowledge, enabling reasoning and judgment (wisdom) by AI agents.\n\n**Definitions**  \n- **Wisdom**: The ability to make judgments and decisions based on knowledge.  \n- **Ontology**: A structured framework to map data to real-world enterprise entities.\n\n**Key Facts**  \n- Foundry IQ is named for its focus on knowledge bases.  \n- Fabric IQ shortcuts OneLake data via ontology for enterprise mapping.  \n- Work IQ enhances user context and inferencing in Microsoft 365.\n\n**Examples**  \n- AI agents using Work IQ for user context, Fabric IQ for system data, and Foundry IQ for organizational knowledge can answer complex questions effectively.\n\n**Exam Tips \ud83c\udfaf**  \n- Understand the distinct roles and integration of Foundry IQ, Fabric IQ, and Work IQ.  \n- Know the data-information-knowledge-wisdom hierarchy and how IQs fit into it.  \n- Be able to explain how combining multiple IQs enhances AI agent capabilities."
  }
]